{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee88182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parlai in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
      "Requirement already satisfied: py-gfm in /usr/local/lib/python3.8/dist-packages (from parlai) (1.0.2)\n",
      "Requirement already satisfied: pandas in /root/.local/lib/python3.8/site-packages (from parlai) (1.3.3)\n",
      "Requirement already satisfied: torchtext>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from parlai) (0.11.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from parlai) (1.21.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from parlai) (2022.1.18)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.8/dist-packages (from parlai) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in /root/.local/lib/python3.8/site-packages (from parlai) (5.4.1)\n",
      "Requirement already satisfied: Unidecode in /usr/local/lib/python3.8/dist-packages (from parlai) (1.3.3)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from parlai) (7.0.1)\n",
      "Requirement already satisfied: numpy in /root/.local/lib/python3.8/site-packages (from parlai) (1.22.2)\n",
      "Requirement already satisfied: py-rouge in /usr/local/lib/python3.8/dist-packages (from parlai) (1.1)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from parlai) (8.1.0)\n",
      "Requirement already satisfied: hydra-core~=1.1.0 in /usr/local/lib/python3.8/dist-packages (from parlai) (1.1.1)\n",
      "Requirement already satisfied: tqdm~=4.62.1 in /root/.local/lib/python3.8/site-packages (from parlai) (4.62.3)\n",
      "Requirement already satisfied: gitdb2 in /usr/local/lib/python3.8/dist-packages (from parlai) (4.0.2)\n",
      "Requirement already satisfied: myst-parser~=0.12.2 in /usr/local/lib/python3.8/dist-packages (from parlai) (0.12.10)\n",
      "Requirement already satisfied: websocket-server in /usr/local/lib/python3.8/dist-packages (from parlai) (0.6.4)\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.8/dist-packages (from parlai) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from parlai) (1.0.2)\n",
      "Requirement already satisfied: docformatter in /usr/local/lib/python3.8/dist-packages (from parlai) (1.4)\n",
      "Requirement already satisfied: pytest-regressions in /usr/local/lib/python3.8/dist-packages (from parlai) (2.3.1)\n",
      "Requirement already satisfied: pexpect in /usr/lib/python3/dist-packages (from parlai) (4.6.0)\n",
      "Requirement already satisfied: Sphinx~=2.2.0 in /usr/local/lib/python3.8/dist-packages (from parlai) (2.2.2)\n",
      "Requirement already satisfied: GitPython in /usr/local/lib/python3.8/dist-packages (from parlai) (3.1.27)\n",
      "Requirement already satisfied: datasets>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from parlai) (1.18.3)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.8/dist-packages (from parlai) (22.3.0)\n",
      "Requirement already satisfied: sh in /usr/local/lib/python3.8/dist-packages (from parlai) (1.14.2)\n",
      "Requirement already satisfied: typing-extensions in /root/.local/lib/python3.8/site-packages (from parlai) (3.10.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from parlai) (1.8.0)\n",
      "Requirement already satisfied: importlib-metadata<4.3 in /usr/local/lib/python3.8/dist-packages (from parlai) (4.2.0)\n",
      "Requirement already satisfied: fairscale in /usr/local/lib/python3.8/dist-packages (from parlai) (0.4.5)\n",
      "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.8/dist-packages (from parlai) (0.3.8)\n",
      "Requirement already satisfied: tensorboard in /root/.local/lib/python3.8/site-packages (from parlai) (2.8.0)\n",
      "Requirement already satisfied: omegaconf~=2.1.1 in /usr/local/lib/python3.8/dist-packages (from parlai) (2.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from parlai) (1.1.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from parlai) (15.0.1)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.8/dist-packages (from parlai) (1.24.8)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.8/dist-packages (from parlai) (1.26.8)\n",
      "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.8/dist-packages (from parlai) (1.0.0)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (from parlai) (1.6.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from parlai) (9.0.1)\n",
      "Requirement already satisfied: flake8 in /usr/local/lib/python3.8/dist-packages (from parlai) (4.0.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from parlai) (3.7)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.8/dist-packages (from parlai) (6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.14 in /usr/local/lib/python3.8/dist-packages (from parlai) (0.15.2)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (from parlai) (2.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from parlai) (2.22.0)\n",
      "Requirement already satisfied: iopath~=0.1.8 in /usr/local/lib/python3.8/dist-packages (from parlai) (0.1.9)\n",
      "Requirement already satisfied: sphinx-autodoc-typehints~=1.10.3 in /usr/local/lib/python3.8/dist-packages (from parlai) (1.10.3)\n",
      "Requirement already satisfied: requests-mock in /usr/local/lib/python3.8/dist-packages (from parlai) (1.9.3)\n",
      "Requirement already satisfied: tokenizers>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from parlai) (0.11.5)\n",
      "Requirement already satisfied: flake8-bugbear in /usr/local/lib/python3.8/dist-packages (from parlai) (22.1.11)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from parlai) (1.10.2)\n",
      "Requirement already satisfied: attrs~=20.2.0 in /usr/local/lib/python3.8/dist-packages (from parlai) (20.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets>=1.4.1->parlai) (0.70.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.4.1->parlai) (0.4.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /root/.local/lib/python3.8/site-packages (from datasets>=1.4.1->parlai) (2022.2.0)\n",
      "Requirement already satisfied: aiohttp in /root/.local/lib/python3.8/site-packages (from datasets>=1.4.1->parlai) (3.8.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.4.1->parlai) (7.0.0)\n",
      "Requirement already satisfied: packaging in /root/.local/lib/python3.8/site-packages (from datasets>=1.4.1->parlai) (21.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets>=1.4.1->parlai) (0.3.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets>=1.4.1->parlai) (3.0.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.8/dist-packages (from hydra-core~=1.1.0->parlai) (4.8)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core~=1.1.0->parlai) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.3->parlai) (3.7.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from iopath~=0.1.8->parlai) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py~=0.5.4 in /usr/local/lib/python3.8/dist-packages (from myst-parser~=0.12.2->parlai) (0.5.8)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (1.0.2)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (1.3.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (1.1.5)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (2.11.2)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (1.0.2)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (2.9.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (1.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /usr/lib/python3/dist-packages (from Sphinx~=2.2.0->parlai) (2.10.1)\n",
      "Requirement already satisfied: setuptools in /root/.local/lib/python3.8/site-packages (from Sphinx~=2.2.0->parlai) (58.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (1.0.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (0.7.12)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.8/dist-packages (from Sphinx~=2.2.0->parlai) (2.2.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from boto3->parlai) (0.5.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->parlai) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /root/.local/lib/python3.8/site-packages (from botocore->parlai) (2.8.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->parlai) (10.0)\n",
      "Requirement already satisfied: untokenize in /usr/local/lib/python3.8/dist-packages (from docformatter->parlai) (0.1.1)\n",
      "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from flake8->parlai) (2.4.0)\n",
      "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from flake8->parlai) (2.8.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from flake8->parlai) (0.6.1)\n",
      "Requirement already satisfied: gitdb>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb2->parlai) (4.0.9)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (3.0.28)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->parlai) (5.1.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->parlai) (7.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /root/.local/lib/python3.8/site-packages (from pandas->parlai) (2021.3)\n",
      "Requirement already satisfied: markdown~=3.2 in /usr/local/lib/python3.8/dist-packages (from py-gfm->parlai) (3.3.4)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest->parlai) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/dist-packages (from pytest->parlai) (1.1.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->parlai) (2.0.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from pytest->parlai) (1.11.0)\n",
      "Requirement already satisfied: pytest-datadir>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pytest-regressions->parlai) (1.3.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from requests-mock->parlai) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->parlai) (3.1.0)\n",
      "Requirement already satisfied: mock in /usr/local/lib/python3.8/dist-packages (from subword-nmt->parlai) (4.0.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (2.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard->parlai) (0.34.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (3.14.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (1.44.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /root/.local/lib/python3.8/site-packages (from tensorboard->parlai) (0.4.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb>=4.0.1->gitdb2->parlai) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->parlai) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard->parlai) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->parlai) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /root/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->parlai) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.4.1->parlai) (3.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->parlai) (0.8.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/.local/lib/python3.8/site-packages (from packaging->datasets>=1.4.1->parlai) (3.0.7)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->parlai) (0.2.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.local/lib/python3.8/site-packages (from aiohttp->datasets>=1.4.1->parlai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/.local/lib/python3.8/site-packages (from aiohttp->datasets>=1.4.1->parlai) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /root/.local/lib/python3.8/site-packages (from aiohttp->datasets>=1.4.1->parlai) (2.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/.local/lib/python3.8/site-packages (from aiohttp->datasets>=1.4.1->parlai) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/.local/lib/python3.8/site-packages (from aiohttp->datasets>=1.4.1->parlai) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/.local/lib/python3.8/site-packages (from aiohttp->datasets>=1.4.1->parlai) (1.3.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->parlai) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->parlai) (2.0.5)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->parlai) (0.8.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->parlai) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard->parlai) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets>=1.4.1->parlai) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f24516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4768a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q subword_nmt # extra requirement we need for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aec94d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:01:01 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.8/dist-packages/data/models/dodecadialogue/empathetic_dialogues_ft/model (previously: data/models/dodecadialogue/empathetic_dialogues/model)\u001b[0m\n",
      "17:01:01 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
      "17:01:01 | loading dictionary from /usr/local/lib/python3.8/dist-packages/data/models/dodecadialogue/empathetic_dialogues_ft/model.dict\n",
      "17:01:02 | num words = 54946\n",
      "17:01:02 | ImageSeq2seq: full interactive mode on.\n",
      "17:01:02 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "17:01:03 | Total parameters: 88,559,104 (88,559,104 trainable)\n",
      "17:01:03 | Loading existing model params from /usr/local/lib/python3.8/dist-packages/data/models/dodecadialogue/empathetic_dialogues_ft/model\n",
      "17:01:06 | Opt:\n",
      "17:01:06 |     activation: gelu\n",
      "17:01:06 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "17:01:06 |     adam_eps: 1e-08\n",
      "17:01:06 |     add_p1_after_newln: False\n",
      "17:01:06 |     aggregate_micro: False\n",
      "17:01:06 |     allow_missing_init_opts: False\n",
      "17:01:06 |     attention_dropout: 0.0\n",
      "17:01:06 |     batch_length_range: 5\n",
      "17:01:06 |     batch_sort_cache_type: pop\n",
      "17:01:06 |     batch_sort_field: text\n",
      "17:01:06 |     batchsize: 16\n",
      "17:01:06 |     beam_block_full_context: False\n",
      "17:01:06 |     beam_block_list_filename: None\n",
      "17:01:06 |     beam_block_ngram: -1\n",
      "17:01:06 |     beam_context_block_ngram: -1\n",
      "17:01:06 |     beam_delay: 30\n",
      "17:01:06 |     beam_length_penalty: 0.65\n",
      "17:01:06 |     beam_min_length: 1\n",
      "17:01:06 |     beam_size: 1\n",
      "17:01:06 |     betas: '[0.9, 0.999]'\n",
      "17:01:06 |     bpe_add_prefix_space: None\n",
      "17:01:06 |     bpe_debug: False\n",
      "17:01:06 |     bpe_dropout: None\n",
      "17:01:06 |     bpe_merge: None\n",
      "17:01:06 |     bpe_vocab: None\n",
      "17:01:06 |     checkpoint_activations: False\n",
      "17:01:06 |     compute_tokenized_bleu: False\n",
      "17:01:06 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "17:01:06 |     datatype: train\n",
      "17:01:06 |     delimiter: '\\n'\n",
      "17:01:06 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "17:01:06 |     dict_endtoken: __end__\n",
      "17:01:06 |     dict_file: /usr/local/lib/python3.8/dist-packages/data/models/dodecadialogue/empathetic_dialogues_ft/model.dict\n",
      "17:01:06 |     dict_include_test: False\n",
      "17:01:06 |     dict_include_valid: False\n",
      "17:01:06 |     dict_initpath: None\n",
      "17:01:06 |     dict_language: english\n",
      "17:01:06 |     dict_loaded: True\n",
      "17:01:06 |     dict_lower: True\n",
      "17:01:06 |     dict_max_ngram_size: -1\n",
      "17:01:06 |     dict_maxexs: -1\n",
      "17:01:06 |     dict_maxtokens: -1\n",
      "17:01:06 |     dict_minfreq: 0\n",
      "17:01:06 |     dict_nulltoken: __null__\n",
      "17:01:06 |     dict_starttoken: __start__\n",
      "17:01:06 |     dict_textfields: text,labels\n",
      "17:01:06 |     dict_tokenizer: bpe\n",
      "17:01:06 |     dict_unktoken: __unk__\n",
      "17:01:06 |     display_add_fields: \n",
      "17:01:06 |     display_examples: False\n",
      "17:01:06 |     display_prettify: False\n",
      "17:01:06 |     download_path: None\n",
      "17:01:06 |     dropout: 0.1\n",
      "17:01:06 |     dynamic_batching: None\n",
      "17:01:06 |     embedding_projection: random\n",
      "17:01:06 |     embedding_size: 512\n",
      "17:01:06 |     embedding_type: random\n",
      "17:01:06 |     embeddings_scale: True\n",
      "17:01:06 |     eval_batchsize: None\n",
      "17:01:06 |     evaltask: None\n",
      "17:01:06 |     ffn_size: 2048\n",
      "17:01:06 |     force_fp16_tokens: False\n",
      "17:01:06 |     fp16: False\n",
      "17:01:06 |     fp16_impl: safe\n",
      "17:01:06 |     gpu: -1\n",
      "17:01:06 |     gradient_clip: 0.1\n",
      "17:01:06 |     hide_labels: False\n",
      "17:01:06 |     history_add_global_end_token: None\n",
      "17:01:06 |     history_reversed: False\n",
      "17:01:06 |     history_size: -1\n",
      "17:01:06 |     image_cropsize: 224\n",
      "17:01:06 |     image_encoder_num_layers: 1\n",
      "17:01:06 |     image_features_dim: 2048\n",
      "17:01:06 |     image_fusion_type: late\n",
      "17:01:06 |     image_mode: none\n",
      "17:01:06 |     image_size: 256\n",
      "17:01:06 |     include_image_token: True\n",
      "17:01:06 |     inference: greedy\n",
      "17:01:06 |     init_model: data/models/dodecadialogue/base_model/model\n",
      "17:01:06 |     init_opt: None\n",
      "17:01:06 |     interactive_mode: True\n",
      "17:01:06 |     interactive_task: True\n",
      "17:01:06 |     invsqrt_lr_decay_gamma: -1\n",
      "17:01:06 |     is_debug: False\n",
      "17:01:06 |     label_truncate: 128\n",
      "17:01:06 |     learn_positional_embeddings: True\n",
      "17:01:06 |     learningrate: 1e-07\n",
      "17:01:06 |     local_human_candidates_file: None\n",
      "17:01:06 |     log_every_n_secs: 10.0\n",
      "17:01:06 |     log_keep_fields: all\n",
      "17:01:06 |     loglevel: info\n",
      "17:01:06 |     lr_scheduler: reduceonplateau\n",
      "17:01:06 |     lr_scheduler_decay: 0.5\n",
      "17:01:06 |     lr_scheduler_patience: 3\n",
      "17:01:06 |     max_train_time: 84600.0\n",
      "17:01:06 |     metrics: default\n",
      "17:01:06 |     model: image_seq2seq\n",
      "17:01:06 |     model_file: /usr/local/lib/python3.8/dist-packages/data/models/dodecadialogue/empathetic_dialogues_ft/model\n",
      "17:01:06 |     model_parallel: False\n",
      "17:01:06 |     momentum: 0\n",
      "17:01:06 |     multitask_weights: [1]\n",
      "17:01:06 |     n_decoder_layers: -1\n",
      "17:01:06 |     n_encoder_layers: -1\n",
      "17:01:06 |     n_heads: 16\n",
      "17:01:06 |     n_image_channels: 1\n",
      "17:01:06 |     n_image_tokens: 1\n",
      "17:01:06 |     n_layers: 8\n",
      "17:01:06 |     n_positions: 512\n",
      "17:01:06 |     n_segments: 0\n",
      "17:01:06 |     nesterov: True\n",
      "17:01:06 |     no_cuda: False\n",
      "17:01:06 |     num_epochs: -1\n",
      "17:01:06 |     numthreads: 1\n",
      "17:01:06 |     numworkers: 4\n",
      "17:01:06 |     nus: [0.7]\n",
      "17:01:06 |     optimizer: adamax\n",
      "17:01:06 |     outfile: \n",
      "17:01:06 |     output_scaling: 1.0\n",
      "17:01:06 |     override: \"{'model_file': '/usr/local/lib/python3.8/dist-packages/data/models/dodecadialogue/empathetic_dialogues_ft/model'}\"\n",
      "17:01:06 |     parlai_home: /checkpoint/kshuster/projects/parlall/parlall_MT_plus_FT/parlall_MT_plus_FT_sweep1_Tue_Oct_29/ParlAI\n",
      "17:01:06 |     person_tokens: False\n",
      "17:01:06 |     pytorch_context_length: -1\n",
      "17:01:06 |     pytorch_datapath: None\n",
      "17:01:06 |     pytorch_include_labels: True\n",
      "17:01:06 |     pytorch_preprocess: False\n",
      "17:01:06 |     pytorch_teacher_batch_sort: False\n",
      "17:01:06 |     pytorch_teacher_dataset: None\n",
      "17:01:06 |     pytorch_teacher_task: None\n",
      "17:01:06 |     rank_candidates: False\n",
      "17:01:06 |     relu_dropout: 0.0\n",
      "17:01:06 |     save_after_valid: True\n",
      "17:01:06 |     save_every_n_secs: -1\n",
      "17:01:06 |     save_format: conversations\n",
      "17:01:06 |     share_word_embeddings: True\n",
      "17:01:06 |     short_final_eval: False\n",
      "17:01:06 |     show_advanced_args: False\n",
      "17:01:06 |     shuffle: False\n",
      "17:01:06 |     single_turn: False\n",
      "17:01:06 |     skip_generation: True\n",
      "17:01:06 |     special_tok_lst: None\n",
      "17:01:06 |     split_lines: False\n",
      "17:01:06 |     starttime: Oct29_07-56\n",
      "17:01:06 |     task: empathetic_dialogues\n",
      "17:01:06 |     temperature: 1.0\n",
      "17:01:06 |     tensorboard_log: False\n",
      "17:01:06 |     text_truncate: 512\n",
      "17:01:06 |     topk: 10\n",
      "17:01:06 |     topp: 0.9\n",
      "17:01:06 |     truncate: -1\n",
      "17:01:06 |     update_freq: 1\n",
      "17:01:06 |     use_reply: label\n",
      "17:01:06 |     validation_cutoff: 1.0\n",
      "17:01:06 |     validation_every_n_epochs: -1\n",
      "17:01:06 |     validation_every_n_secs: 3600.0\n",
      "17:01:06 |     validation_max_exs: -1\n",
      "17:01:06 |     validation_metric: ppl\n",
      "17:01:06 |     validation_metric_mode: min\n",
      "17:01:06 |     validation_patience: 10\n",
      "17:01:06 |     validation_share_agent: False\n",
      "17:01:06 |     variant: xlm\n",
      "17:01:06 |     verbose: False\n",
      "17:01:06 |     warmup_rate: 0.0001\n",
      "17:01:06 |     warmup_updates: 2000\n",
      "17:01:06 |     weight_decay: None\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "17:01:06 | creating task(s): interactive\n",
      "Enter Your Message: I am feeling scared\n",
      "\u001b[0;34m[ImageSeq2seq]:\u001b[0;0m \u001b[1mwhy is that ?\u001b[0;0m\n",
      "Enter Your Message: a war is going on my country\n",
      "\u001b[0;34m[ImageSeq2seq]:\u001b[0;0m \u001b[1mi am sorry to hear that .\u001b[0;0m\n",
      "Enter Your Message: what should I do\n",
      "\u001b[0;34m[ImageSeq2seq]:\u001b[0;0m \u001b[1mi am not sure .\u001b[0;0m\n",
      "Enter Your Message: [EXIT]\n",
      "CHAT DONE \n"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.interactive import Interactive\n",
    "Interactive.main(model_file='zoo:dodecadialogue/empathetic_dialogues_ft/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4b157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:01:51 | Opt:\n",
      "17:01:51 |     allow_missing_init_opts: False\n",
      "17:01:51 |     batchsize: 1\n",
      "17:01:51 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "17:01:51 |     datatype: train:ordered\n",
      "17:01:51 |     dict_class: None\n",
      "17:01:51 |     display_add_fields: \n",
      "17:01:51 |     download_path: None\n",
      "17:01:51 |     dynamic_batching: None\n",
      "17:01:51 |     hide_labels: False\n",
      "17:01:51 |     ignore_agent_reply: True\n",
      "17:01:51 |     image_cropsize: 224\n",
      "17:01:51 |     image_mode: raw\n",
      "17:01:51 |     image_size: 256\n",
      "17:01:51 |     init_model: None\n",
      "17:01:51 |     init_opt: None\n",
      "17:01:52 |     is_debug: False\n",
      "17:01:52 |     loglevel: info\n",
      "17:01:52 |     max_display_len: 1000\n",
      "17:01:52 |     model: None\n",
      "17:01:52 |     model_file: None\n",
      "17:01:52 |     multitask_weights: [1]\n",
      "17:01:52 |     mutators: None\n",
      "17:01:52 |     num_examples: 5\n",
      "17:01:52 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
      "17:01:52 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "17:01:52 |     remove_political_convos: False\n",
      "17:01:52 |     starttime: Feb28_17-01\n",
      "17:01:52 |     task: empathetic_dialogues\n",
      "17:01:52 |     train_experiencer_only: False\n",
      "17:01:52 |     verbose: False\n",
      "17:01:52 | creating task(s): empathetic_dialogues\n",
      "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
      "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
      "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
      "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
      "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
      "\u001b[0mWe no longer talk.\u001b[0;0m\n",
      "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
      "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
      "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
      "\u001b[0mWhere has she gone?\u001b[0;0m\n",
      "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
      "17:01:52 | loaded 39057 episodes with a total of 64636 examples\n"
     ]
    }
   ],
   "source": [
    "# The display_data script is used to show the contents of a particular task.\n",
    "# By default, we show the train\n",
    "from parlai.scripts.display_data import DisplayData\n",
    "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082b048",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple transformer/generator with attention, to respond to blended_skill talk dialogues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9075c557",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:44 | building dictionary first...\n",
      "17:23:44 | No model with opt yet at: from_pretrained/model(.opt)\n",
      "17:23:44 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,is_debug: False,datapath: /usr/local/lib/python3.8/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: True,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,interactive_mode: False,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None\u001b[0m\n",
      "17:23:44 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --max-train-time -1 --save-every-n-secs 60.0 --save-after-valid True --validation-max-exs 20000 --validation-patience 15 --validation-metric-mode min --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
      "17:23:44 | loading dictionary from /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict\n",
      "17:23:44 | num words = 54944\n",
      "17:23:45 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "17:23:45 | Loading existing model params from /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "17:23:45 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
      "17:23:45 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
      "17:23:45 | Opt:\n",
      "17:23:45 |     activation: gelu\n",
      "17:23:45 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "17:23:45 |     adam_eps: 1e-08\n",
      "17:23:45 |     add_p1_after_newln: False\n",
      "17:23:45 |     aggregate_micro: False\n",
      "17:23:45 |     allow_missing_init_opts: False\n",
      "17:23:45 |     attention_dropout: 0.0\n",
      "17:23:45 |     batchsize: 24\n",
      "17:23:45 |     beam_block_full_context: True\n",
      "17:23:45 |     beam_block_list_filename: None\n",
      "17:23:45 |     beam_block_ngram: -1\n",
      "17:23:45 |     beam_context_block_ngram: -1\n",
      "17:23:45 |     beam_delay: 30\n",
      "17:23:45 |     beam_length_penalty: 0.65\n",
      "17:23:45 |     beam_min_length: 1\n",
      "17:23:45 |     beam_size: 1\n",
      "17:23:45 |     betas: '(0.9, 0.999)'\n",
      "17:23:45 |     bpe_add_prefix_space: None\n",
      "17:23:45 |     bpe_debug: False\n",
      "17:23:45 |     bpe_dropout: None\n",
      "17:23:45 |     bpe_merge: None\n",
      "17:23:45 |     bpe_vocab: None\n",
      "17:23:45 |     checkpoint_activations: False\n",
      "17:23:45 |     compute_tokenized_bleu: False\n",
      "17:23:45 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "17:23:45 |     datatype: train\n",
      "17:23:45 |     delimiter: '\\n'\n",
      "17:23:45 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "17:23:45 |     dict_endtoken: __end__\n",
      "17:23:45 |     dict_file: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict\n",
      "17:23:45 |     dict_include_test: False\n",
      "17:23:45 |     dict_include_valid: False\n",
      "17:23:45 |     dict_initpath: None\n",
      "17:23:45 |     dict_language: english\n",
      "17:23:45 |     dict_loaded: True\n",
      "17:23:45 |     dict_lower: True\n",
      "17:23:45 |     dict_max_ngram_size: -1\n",
      "17:23:45 |     dict_maxexs: -1\n",
      "17:23:45 |     dict_maxtokens: -1\n",
      "17:23:45 |     dict_minfreq: 0\n",
      "17:23:45 |     dict_nulltoken: __null__\n",
      "17:23:45 |     dict_starttoken: __start__\n",
      "17:23:45 |     dict_textfields: text,labels\n",
      "17:23:45 |     dict_tokenizer: bpe\n",
      "17:23:45 |     dict_unktoken: __unk__\n",
      "17:23:45 |     display_examples: False\n",
      "17:23:45 |     download_path: None\n",
      "17:23:45 |     dropout: 0.0\n",
      "17:23:45 |     dynamic_batching: full\n",
      "17:23:45 |     embedding_projection: random\n",
      "17:23:45 |     embedding_size: 512\n",
      "17:23:45 |     embedding_type: random\n",
      "17:23:45 |     embeddings_scale: True\n",
      "17:23:45 |     eval_batchsize: None\n",
      "17:23:45 |     eval_dynamic_batching: None\n",
      "17:23:45 |     evaltask: None\n",
      "17:23:45 |     ffn_size: 2048\n",
      "17:23:45 |     final_extra_opt: \n",
      "17:23:45 |     force_fp16_tokens: False\n",
      "17:23:45 |     fp16: True\n",
      "17:23:45 |     fp16_impl: safe\n",
      "17:23:45 |     gpu: -1\n",
      "17:23:45 |     gradient_clip: 0.1\n",
      "17:23:45 |     hide_labels: False\n",
      "17:23:45 |     history_add_global_end_token: None\n",
      "17:23:45 |     history_reversed: False\n",
      "17:23:45 |     history_size: -1\n",
      "17:23:45 |     image_cropsize: 224\n",
      "17:23:45 |     image_mode: raw\n",
      "17:23:45 |     image_size: 256\n",
      "17:23:45 |     inference: greedy\n",
      "17:23:45 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "17:23:45 |     init_opt: None\n",
      "17:23:45 |     interactive_mode: False\n",
      "17:23:45 |     invsqrt_lr_decay_gamma: -1\n",
      "17:23:45 |     is_debug: False\n",
      "17:23:45 |     label_truncate: 128\n",
      "17:23:45 |     learn_positional_embeddings: True\n",
      "17:23:45 |     learningrate: 1e-05\n",
      "17:23:45 |     load_from_checkpoint: True\n",
      "17:23:45 |     log_every_n_secs: -1\n",
      "17:23:45 |     log_every_n_steps: 50\n",
      "17:23:45 |     loglevel: info\n",
      "17:23:45 |     lr_scheduler: reduceonplateau\n",
      "17:23:45 |     lr_scheduler_decay: 0.5\n",
      "17:23:45 |     lr_scheduler_patience: 3\n",
      "17:23:45 |     max_train_steps: -1\n",
      "17:23:45 |     max_train_time: 3600.0\n",
      "17:23:45 |     metrics: default\n",
      "17:23:45 |     model: transformer/generator\n",
      "17:23:45 |     model_file: from_pretrained/model\n",
      "17:23:45 |     model_parallel: True\n",
      "17:23:45 |     momentum: 0\n",
      "17:23:45 |     multitask_weights: [1]\n",
      "17:23:45 |     mutators: None\n",
      "17:23:45 |     n_decoder_layers: -1\n",
      "17:23:45 |     n_encoder_layers: -1\n",
      "17:23:45 |     n_heads: 16\n",
      "17:23:45 |     n_layers: 8\n",
      "17:23:45 |     n_positions: 512\n",
      "17:23:45 |     n_segments: 0\n",
      "17:23:45 |     nesterov: True\n",
      "17:23:45 |     no_cuda: False\n",
      "17:23:45 |     num_epochs: -1\n",
      "17:23:45 |     num_workers: 0\n",
      "17:23:45 |     nus: (0.7,)\n",
      "17:23:45 |     optimizer: adamax\n",
      "17:23:45 |     output_scaling: 1.0\n",
      "17:23:45 |     override: \"{'task': 'blended_skill_talk', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:blender/blender_90M/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'model_parallel': True, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adamax', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 3600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 24, 'fp16': True, 'fp16_impl': 'safe', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
      "17:23:45 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "17:23:45 |     person_tokens: False\n",
      "17:23:45 |     rank_candidates: False\n",
      "17:23:45 |     relu_dropout: 0.0\n",
      "17:23:45 |     save_after_valid: False\n",
      "17:23:45 |     save_every_n_secs: -1\n",
      "17:23:45 |     share_word_embeddings: True\n",
      "17:23:45 |     short_final_eval: False\n",
      "17:23:45 |     skip_generation: True\n",
      "17:23:45 |     special_tok_lst: None\n",
      "17:23:45 |     split_lines: False\n",
      "17:23:45 |     starttime: Feb28_17-23\n",
      "17:23:45 |     task: blended_skill_talk\n",
      "17:23:45 |     temperature: 1.0\n",
      "17:23:45 |     tensorboard_log: False\n",
      "17:23:45 |     tensorboard_logdir: None\n",
      "17:23:45 |     text_truncate: 512\n",
      "17:23:45 |     topk: 10\n",
      "17:23:45 |     topp: 0.9\n",
      "17:23:45 |     truncate: -1\n",
      "17:23:45 |     update_freq: 1\n",
      "17:23:45 |     use_reply: label\n",
      "17:23:45 |     validation_cutoff: 1.0\n",
      "17:23:45 |     validation_every_n_epochs: 0.25\n",
      "17:23:45 |     validation_every_n_secs: -1\n",
      "17:23:45 |     validation_every_n_steps: -1\n",
      "17:23:45 |     validation_max_exs: -1\n",
      "17:23:45 |     validation_metric: ppl\n",
      "17:23:45 |     validation_metric_mode: None\n",
      "17:23:45 |     validation_patience: 10\n",
      "17:23:45 |     validation_share_agent: False\n",
      "17:23:45 |     variant: xlm\n",
      "17:23:45 |     verbose: False\n",
      "17:23:45 |     wandb_entity: None\n",
      "17:23:45 |     wandb_log: False\n",
      "17:23:45 |     wandb_name: None\n",
      "17:23:45 |     wandb_project: None\n",
      "17:23:45 |     warmup_rate: 0.0001\n",
      "17:23:45 |     warmup_updates: 100\n",
      "17:23:45 |     weight_decay: None\n",
      "17:23:46 | creating task(s): blended_skill_talk\n",
      "17:23:46 | Loading ParlAI text data: /usr/local/lib/python3.8/dist-packages/data/blended_skill_talk/train.txt\n",
      "17:23:48 | training...\n",
      "17:25:39 | time:111s total_exs:4520 total_steps:50 epochs:0.17\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   125.5     1 11342  5134       0          0 40.92 4520  4.364 17.84 2.231 5.001e-06  1613 730.1       0          0 9.309   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4948  .0006637                   50 12955 5864 .4526\n",
      "\n",
      "17:27:05 | time:197s total_exs:6812 total_steps:83 epochs:0.25\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   173.4     1 12043  4595       0          0  26.5 2292  4.928 18.23 2.252 8.3e-06  1266 482.9       0          0 9.506   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4936         0                   83 13309 5077 .3815\n",
      "\n",
      "17:27:05 | creating task(s): blended_skill_talk\n",
      "17:27:05 | Loading ParlAI text data: /usr/local/lib/python3.8/dist-packages/data/blended_skill_talk/valid.txt\n",
      "17:27:07 | running eval: valid\n",
      "17:28:03 | eval completed in 56.63s\n",
      "17:28:03 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11636 16310       0          0 100.3 5651 18.14 2.675 8.3e-06  1298  1819       0          0 14.52      .4298   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "     .000177                   83 12934 18129\n",
      "\u001b[0m\n",
      "17:28:03 | \u001b[1;32mnew best ppl: 14.52\u001b[0m\n",
      "17:28:03 | saving best valid model: from_pretrained/model\n",
      "17:28:03 | Saving dictionary to from_pretrained/model.dict\n",
      "17:30:10 | time:382s total_exs:10556 total_steps:133 epochs:0.39\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   156.3     1 11700  4671       0          0  29.9 3744  4.813 17.96 2.253 9.9e-06  1345 536.9       0          0 9.514   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4929  .0008013                  133 13045 5208 .3993\n",
      "\n",
      "17:31:58 | time:490s total_exs:13640 total_steps:176 epochs:0.50\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "     164     1 11760  4683       0          0 28.56 3084  4.869 17.83 2.226 9.9e-06  1279 509.1       0          0 9.26   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4961  .0003243                  176 13039 5192 .3982\n",
      "\n",
      "17:31:58 | running eval: valid\n",
      "17:32:56 | eval completed in 58.19s\n",
      "17:32:56 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11636 15863       0          0 97.52 5651 18.14 2.682 9.9e-06  1298  1769       0          0 14.61      .4287   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "    .0003539                  176 12934 17632\n",
      "\u001b[0m\n",
      "17:32:56 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 1\u001b[0m\n",
      "17:35:11 | time:683s total_exs:17380 total_steps:226 epochs:0.64\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   156.6     1 11713  4346       0          0 27.76 3740  4.806 17.44 2.208 9.9e-06  1304   484       0          0 9.094   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4965         0                  226 13017 4830 .3711\n",
      "\n",
      "17:37:26 | time:819s total_exs:20500 total_steps:270 epochs:0.76\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "     170     1 12053  3912       0          0 23.02 3120  4.907  17.9 2.218 9.9e-06  1269 411.9       0          0 9.192   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4968  .0009615                  270 13322 4324 .3246\n",
      "\n",
      "17:37:26 | running eval: valid\n",
      "17:38:34 | eval completed in 67.84s\n",
      "17:38:34 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 13595       0          0 83.58 5651 18.14 2.684 9.9e-06  1282  1516       0          0 14.64      .4287   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "     .000177                  270 12772 15112\n",
      "\u001b[0m\n",
      "17:38:34 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 2\u001b[0m\n",
      "17:40:53 | time:1025s total_exs:24196 total_steps:320 epochs:0.90\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   156.8     1 11592  4170       0          0 26.59 3696  4.743 18.25 2.217 9.9e-06  1349 485.3       0          0 9.175   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4934  .0005411                  320 12941 4655 .3598\n",
      "\n",
      "17:42:53 | time:1145s total_exs:27280 total_steps:362 epochs:1.01\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   159.5     1 11712  4103       0          0 25.72 3084  4.796 18.26 2.202 9.9e-06  1341 469.8       0          0 9.04   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4953  .0003243                  362 13053 4573 .3503\n",
      "\n",
      "17:42:53 | running eval: valid\n",
      "17:43:47 | eval completed in 53.39s\n",
      "17:43:47 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11636 17297       0          0 106.3 5651 18.14 2.684 9.9e-06  1298  1929       0          0 14.64      .4286   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "     .000177                  362 12934 19226\n",
      "\u001b[0m\n",
      "17:43:47 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 3\u001b[0m\n",
      "17:46:10 | time:1343s total_exs:30740 total_steps:412 epochs:1.14\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   171.5     1 11867  4129       0          0 24.08 3460  4.848 18.39 2.193 9.9e-06  1273 442.9       0          0 8.962   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4997   .000578                  412 13140 4572 .3479\n",
      "\n",
      "17:48:24 | time:1477s total_exs:34056 total_steps:456 epochs:1.26\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   157.4     1 11862  3895       0          0 24.75 3316  4.846 17.56 2.203 9.9e-06  1323 434.5       0          0 9.056   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4974  .0003016                  456 13185 4330 .3284\n",
      "\n",
      "17:48:24 | running eval: valid\n",
      "17:49:12 | eval completed in 47.80s\n",
      "17:49:12 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 19328       0          0 118.8 5651 18.14 2.686 9.9e-06  1282  2156       0          0 14.68      .4283   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "    .0003539                  456 12772 21484\n",
      "\u001b[0m\n",
      "17:49:12 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 4\u001b[0m\n",
      "17:51:31 | time:1663s total_exs:37712 total_steps:506 epochs:1.40\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   160.6     1 11740  4226       0          0 26.32 3656  4.888 18.02 2.236 9.9e-06  1318 474.2       0          0 9.355   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4963  .0002735                  506 13058 4700 .3599\n",
      "\n",
      "17:53:24 | time:1776s total_exs:40852 total_steps:548 epochs:1.51\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   154.3     1 11534  4283       0          0 27.77 3140   4.69 18.19 2.184 9.9e-06  1360   505       0          0 8.883   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4996         0                  548 12893 4788 .3714\n",
      "\n",
      "17:53:24 | running eval: valid\n",
      "17:54:11 | eval completed in 46.47s\n",
      "17:54:11 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 19881       0          0 122.2 5651 18.14 2.689 9.9e-06  1282  2218       0          0 14.72      .4284   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "     .000177                  548 12772 22098\n",
      "\u001b[0m\n",
      "Epoch     5: reducing learning rate of group 0 to 4.9500e-06.\n",
      "17:54:11 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 5\u001b[0m\n",
      "17:56:30 | time:1962s total_exs:44444 total_steps:598 epochs:1.64\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   164.6     1 11828  4239       0          0 25.75 3592  4.852 17.73  2.19 4.95e-06  1274 456.5       0          0 8.936   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5032  .0005568                  598 13102 4696 .3584\n",
      "\n",
      "17:58:21 | time:2073s total_exs:47648 total_steps:642 epochs:1.76\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   158.9     1 11569  4609       0          0 29.01 3204  4.813    18 2.188 4.95e-06  1310   522       0          0 8.92   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5010         0                  642 12880 5131 .3984\n",
      "\n",
      "17:58:21 | running eval: valid\n",
      "17:59:08 | eval completed in 47.77s\n",
      "17:59:08 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11349 19337       0          0 118.9 5651 18.14 2.689 4.95e-06  1266  2157       0          0 14.71      .4288   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "     .000177                  642 12615 21494\n",
      "\u001b[0m\n",
      "17:59:08 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 6\u001b[0m\n",
      "18:01:21 | time:2254s total_exs:51168 total_steps:692 epochs:1.89\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   168.8     1 11883  4469       0          0 26.48 3520  4.967 17.95 2.173 4.95e-06  1263 475.2       0          0 8.787   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5014  .0002841                  692 13146 4945 .3762\n",
      "\n",
      "18:03:12 | time:2364s total_exs:54408 total_steps:735 epochs:2.01\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   155.3     1 11703  4546       0          0 29.27 3240  4.685 17.93 2.175 4.95e-06  1351 524.8       0          0 8.805   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5043         0                  735 13054 5070 .3884\n",
      "\n",
      "18:03:12 | running eval: valid\n",
      "18:04:01 | eval completed in 48.50s\n",
      "18:04:01 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 19042       0          0 117.1 5651 18.14 2.691 4.95e-06  1282  2124       0          0 14.75      .4279   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "    .0003539                  735 12772 21166\n",
      "\u001b[0m\n",
      "18:04:01 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 7\u001b[0m\n",
      "18:06:17 | time:2549s total_exs:57868 total_steps:785 epochs:2.14\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   170.7     1 11816  4337       0          0  25.4 3460  4.957 18.65 2.173 4.95e-06  1290 473.6       0          0 8.781   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5034  .0008671                  785 13106 4811 .3671\n",
      "\n",
      "18:08:06 | time:2658s total_exs:61216 total_steps:829 epochs:2.27\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   154.6     1 11762  4755       0          0 30.76 3348  4.727 17.76  2.16 4.95e-06  1351 546.2       0          0 8.675   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5070  .0005974                  829 13113 5301 .4043\n",
      "\n",
      "18:08:06 | running eval: valid\n",
      "18:08:58 | eval completed in 51.93s\n",
      "18:08:58 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 17781       0          0 109.3 5651 18.14 2.693 4.95e-06  1282  1983       0          0 14.78      .4280   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "    .0003539                  829 12772 19765\n",
      "\u001b[0m\n",
      "18:08:58 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 8\u001b[0m\n",
      "18:11:06 | time:2838s total_exs:64784 total_steps:879 epochs:2.40\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   165.3     1 11792  4600       0          0 27.84 3568   4.97 18.05 2.194 4.95e-06  1288 502.5       0          0 8.969   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5015  .0002803                  879 13080 5102 .3901\n",
      "\n",
      "18:13:09 | time:2961s total_exs:68084 total_steps:925 epochs:2.52\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   162.6     1 11664  4369       0          0 26.87 3300  4.894 18.03 2.169 4.95e-06  1293 484.5       0          0 8.748   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5044  .0006061                  925 12958 4853 .3746\n",
      "\n",
      "18:13:09 | running eval: valid\n",
      "18:13:56 | eval completed in 47.68s\n",
      "18:13:56 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "   162.7 11491 19376       0          0 119.1 5651 18.14 2.694 4.95e-06  1282  2161       0          0 14.8      .4279   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "    .0003539                  925 12772 21537\n",
      "\u001b[0m\n",
      "Epoch     9: reducing learning rate of group 0 to 2.4750e-06.\n",
      "18:13:56 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 9\u001b[0m\n",
      "18:15:57 | time:3129s total_exs:71856 total_steps:975 epochs:2.66\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   153.3     1 11568  4795       0          0 31.27 3772  4.757 17.79 2.189 2.475e-06  1342 556.4       0          0 8.927   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5005  .0007953                  975 12910 5352 .4145\n",
      "\n",
      "18:17:50 | time:3242s total_exs:74844 total_steps:1020 epochs:2.77\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   178.6     1 11860  4747       0          0 26.57 2988  5.013 18.68 2.202 2.475e-06  1240 496.4       0          0 9.042   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5028  .0006693                 1020 13101 5243 .4002\n",
      "\n",
      "18:17:50 | running eval: valid\n",
      "18:18:34 | eval completed in 44.51s\n",
      "18:18:34 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 20755       0          0 127.6 5651 18.14 2.693 2.475e-06  1282  2315       0          0 14.77      .4281   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "    .0003539                 1020 12772 23070\n",
      "\u001b[0m\n",
      "18:18:34 | \u001b[1mdid not beat best ppl: 14.5184 impatience: 10\u001b[0m\n",
      "18:18:34 | ran out of patience! stopping training.\n",
      "18:18:34 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model)\u001b[0m\n",
      "18:18:34 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: True,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.8/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
      "18:18:34 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --max-train-time -1 --save-every-n-secs 60.0 --save-after-valid True --validation-max-exs 20000 --validation-patience 15 --validation-metric-mode min --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --force-fp16-tokens False --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
      "18:18:34 | loading dictionary from from_pretrained/model.dict\n",
      "18:18:34 | num words = 54944\n",
      "18:18:35 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "18:18:35 | Loading existing model params from from_pretrained/model\n",
      "18:18:36 | creating task(s): blended_skill_talk\n",
      "18:18:36 | Loading ParlAI text data: /usr/local/lib/python3.8/dist-packages/data/blended_skill_talk/valid.txt\n",
      "18:18:37 | running eval: valid\n",
      "18:19:23 | eval completed in 45.19s\n",
      "18:19:23 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "   162.7 11491 20489       0          0   126 5651 18.14 2.675 8.3e-06  1282  2285       0          0 14.52      .4298   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "     .000177                   83 12772 22774\n",
      "\u001b[0m\n",
      "18:19:23 | creating task(s): blended_skill_talk\n",
      "18:19:23 | Loading ParlAI text data: /usr/local/lib/python3.8/dist-packages/data/blended_skill_talk/test.txt\n",
      "18:19:24 | running eval: test\n",
      "18:20:09 | eval completed in 44.70s\n",
      "18:20:09 | \u001b[1mtest:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "     165 11595 20346       0          0 123.3 5482 18.88 2.669 8.3e-06  1327  2328       0          0 14.42      .4339   \n",
      "    token_em  total_train_updates   tpb   tps  \n",
      "           0                   83 12922 22675\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'exs': SumMetric(5651),\n",
       "  'clen': AverageMetric(162.7),\n",
       "  'ctrunc': AverageMetric(0),\n",
       "  'ctrunclen': AverageMetric(0),\n",
       "  'llen': AverageMetric(18.14),\n",
       "  'ltrunc': AverageMetric(0),\n",
       "  'ltrunclen': AverageMetric(0),\n",
       "  'loss': AverageMetric(2.675),\n",
       "  'ppl': PPLMetric(14.52),\n",
       "  'token_acc': AverageMetric(0.4298),\n",
       "  'token_em': AverageMetric(0.000177),\n",
       "  'exps': GlobalTimerMetric(126),\n",
       "  'ltpb': GlobalAverageMetric(1282),\n",
       "  'ltps': GlobalTimerMetric(2285),\n",
       "  'ctpb': GlobalAverageMetric(1.149e+04),\n",
       "  'ctps': GlobalTimerMetric(2.049e+04),\n",
       "  'tpb': GlobalAverageMetric(1.277e+04),\n",
       "  'tps': GlobalTimerMetric(2.277e+04),\n",
       "  'lr': GlobalAverageMetric(8.3e-06),\n",
       "  'total_train_updates': GlobalFixedMetric(83)},\n",
       " {'exs': SumMetric(5482),\n",
       "  'clen': AverageMetric(165),\n",
       "  'ctrunc': AverageMetric(0),\n",
       "  'ctrunclen': AverageMetric(0),\n",
       "  'llen': AverageMetric(18.88),\n",
       "  'ltrunc': AverageMetric(0),\n",
       "  'ltrunclen': AverageMetric(0),\n",
       "  'loss': AverageMetric(2.669),\n",
       "  'ppl': PPLMetric(14.42),\n",
       "  'token_acc': AverageMetric(0.4339),\n",
       "  'token_em': AverageMetric(0),\n",
       "  'exps': GlobalTimerMetric(123.3),\n",
       "  'ltpb': GlobalAverageMetric(1327),\n",
       "  'ltps': GlobalTimerMetric(2328),\n",
       "  'ctpb': GlobalAverageMetric(1.16e+04),\n",
       "  'ctps': GlobalTimerMetric(2.035e+04),\n",
       "  'tpb': GlobalAverageMetric(1.292e+04),\n",
       "  'tps': GlobalTimerMetric(2.267e+04),\n",
       "  'lr': GlobalAverageMetric(8.3e-06),\n",
       "  'total_train_updates': GlobalFixedMetric(83)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf from_pretrained\n",
    "!mkdir -p from_pretrained\n",
    "from parlai.scripts.train_model import TrainModel\n",
    "TrainModel.main(\n",
    "    # similar to before\n",
    "    task='blended_skill_talk', \n",
    "    model='transformer/generator',\n",
    "    model_file='from_pretrained/model',\n",
    "    \n",
    "    # initialize with a pretrained model\n",
    "    init_model='zoo:blender/blender_90M/model',\n",
    "    \n",
    "    # arguments we get from the pretrained model.\n",
    "    # Unfortunately, these must be looked up separately for each model.\n",
    "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
    "    label_truncate=128, ffn_size=2048, embedding_size=512,model_parallel=True,\n",
    "    activation='gelu', variant='xlm',\n",
    "    dict_lower=True, dict_tokenizer='bpe',\n",
    " dict_file='/usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict',\n",
    "    learn_positional_embeddings=True,\n",
    "    \n",
    "    # some training arguments, specific to this fine-tuning\n",
    "    # use a small learning rate with ADAM optimizer\n",
    "    lr=1e-5, optimizer='adamax',\n",
    "    warmup_updates=100,\n",
    "    # early stopping on perplexity\n",
    "    validation_metric='ppl',\n",
    "    # train at most 60 minutes/1 Hour, and validate every 0.25 epochs\n",
    "    max_train_time=3600, validation_every_n_epochs=0.25,\n",
    "    \n",
    "    # depend on our system. Synce we have pretty good AI accelerator it works fine\n",
    "    batchsize=24, fp16=True, fp16_impl='safe',\n",
    "    \n",
    "    # speeds up validation\n",
    "    skip_generation=True,\n",
    "    \n",
    "    # helps us cram more examples into our accelerator at a time\n",
    "    dynamic_batching='full',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10326b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:21:58 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
      "18:21:58 | loading dictionary from from_pretrained/model.dict\n",
      "18:21:58 | num words = 54944\n",
      "18:21:59 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "18:21:59 | Loading existing model params from from_pretrained/model\n",
      "18:22:00 | creating task(s): blended_skill_talk\n",
      "18:22:00 | Loading ParlAI text data: /usr/local/lib/python3.8/dist-packages/data/blended_skill_talk/valid.txt\n",
      "18:22:01 | Opt:\n",
      "18:22:01 |     activation: gelu\n",
      "18:22:01 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "18:22:01 |     adam_eps: 1e-08\n",
      "18:22:01 |     add_p1_after_newln: False\n",
      "18:22:01 |     aggregate_micro: False\n",
      "18:22:01 |     allow_missing_init_opts: False\n",
      "18:22:01 |     attention_dropout: 0.0\n",
      "18:22:01 |     batchsize: 24\n",
      "18:22:01 |     beam_block_full_context: True\n",
      "18:22:01 |     beam_block_list_filename: None\n",
      "18:22:01 |     beam_block_ngram: -1\n",
      "18:22:01 |     beam_context_block_ngram: -1\n",
      "18:22:01 |     beam_delay: 30\n",
      "18:22:01 |     beam_length_penalty: 0.65\n",
      "18:22:01 |     beam_min_length: 1\n",
      "18:22:01 |     beam_size: 1\n",
      "18:22:01 |     betas: '[0.9, 0.999]'\n",
      "18:22:01 |     bpe_add_prefix_space: None\n",
      "18:22:01 |     bpe_debug: False\n",
      "18:22:01 |     bpe_dropout: None\n",
      "18:22:01 |     bpe_merge: None\n",
      "18:22:01 |     bpe_vocab: None\n",
      "18:22:01 |     checkpoint_activations: False\n",
      "18:22:01 |     compute_tokenized_bleu: False\n",
      "18:22:01 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "18:22:01 |     datatype: train\n",
      "18:22:01 |     delimiter: '\\n'\n",
      "18:22:01 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "18:22:01 |     dict_endtoken: __end__\n",
      "18:22:01 |     dict_file: from_pretrained/model.dict\n",
      "18:22:01 |     dict_include_test: False\n",
      "18:22:01 |     dict_include_valid: False\n",
      "18:22:01 |     dict_initpath: None\n",
      "18:22:01 |     dict_language: english\n",
      "18:22:01 |     dict_loaded: True\n",
      "18:22:01 |     dict_lower: True\n",
      "18:22:01 |     dict_max_ngram_size: -1\n",
      "18:22:01 |     dict_maxexs: -1\n",
      "18:22:01 |     dict_maxtokens: -1\n",
      "18:22:01 |     dict_minfreq: 0\n",
      "18:22:01 |     dict_nulltoken: __null__\n",
      "18:22:01 |     dict_starttoken: __start__\n",
      "18:22:01 |     dict_textfields: text,labels\n",
      "18:22:01 |     dict_tokenizer: bpe\n",
      "18:22:01 |     dict_unktoken: __unk__\n",
      "18:22:01 |     display_add_fields: \n",
      "18:22:01 |     display_examples: False\n",
      "18:22:01 |     download_path: None\n",
      "18:22:01 |     dropout: 0.0\n",
      "18:22:01 |     dynamic_batching: full\n",
      "18:22:01 |     embedding_projection: random\n",
      "18:22:01 |     embedding_size: 512\n",
      "18:22:01 |     embedding_type: random\n",
      "18:22:01 |     embeddings_scale: True\n",
      "18:22:01 |     eval_batchsize: None\n",
      "18:22:01 |     eval_dynamic_batching: None\n",
      "18:22:01 |     evaltask: None\n",
      "18:22:01 |     ffn_size: 2048\n",
      "18:22:01 |     final_extra_opt: \n",
      "18:22:01 |     force_fp16_tokens: True\n",
      "18:22:01 |     fp16: True\n",
      "18:22:01 |     fp16_impl: safe\n",
      "18:22:01 |     gpu: -1\n",
      "18:22:01 |     gradient_clip: 0.1\n",
      "18:22:01 |     hide_labels: False\n",
      "18:22:01 |     history_add_global_end_token: None\n",
      "18:22:01 |     history_reversed: False\n",
      "18:22:01 |     history_size: -1\n",
      "18:22:01 |     image_cropsize: 224\n",
      "18:22:01 |     image_mode: raw\n",
      "18:22:01 |     image_size: 256\n",
      "18:22:01 |     inference: greedy\n",
      "18:22:01 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "18:22:01 |     init_opt: None\n",
      "18:22:01 |     interactive_mode: False\n",
      "18:22:01 |     invsqrt_lr_decay_gamma: -1\n",
      "18:22:01 |     is_debug: False\n",
      "18:22:01 |     label_truncate: 128\n",
      "18:22:01 |     learn_positional_embeddings: True\n",
      "18:22:01 |     learningrate: 1e-05\n",
      "18:22:01 |     log_every_n_secs: -1\n",
      "18:22:01 |     log_every_n_steps: 50\n",
      "18:22:01 |     loglevel: info\n",
      "18:22:01 |     lr_scheduler: reduceonplateau\n",
      "18:22:01 |     lr_scheduler_decay: 0.5\n",
      "18:22:01 |     lr_scheduler_patience: 3\n",
      "18:22:01 |     max_train_steps: -1\n",
      "18:22:01 |     max_train_time: 3600.0\n",
      "18:22:01 |     metrics: default\n",
      "18:22:01 |     model: transformer/generator\n",
      "18:22:01 |     model_file: from_pretrained/model\n",
      "18:22:01 |     model_parallel: True\n",
      "18:22:01 |     momentum: 0\n",
      "18:22:01 |     multitask_weights: [1]\n",
      "18:22:01 |     mutators: None\n",
      "18:22:01 |     n_decoder_layers: -1\n",
      "18:22:01 |     n_encoder_layers: -1\n",
      "18:22:01 |     n_heads: 16\n",
      "18:22:01 |     n_layers: 8\n",
      "18:22:01 |     n_positions: 512\n",
      "18:22:01 |     n_segments: 0\n",
      "18:22:01 |     nesterov: True\n",
      "18:22:01 |     no_cuda: False\n",
      "18:22:01 |     num_epochs: -1\n",
      "18:22:01 |     num_examples: 2\n",
      "18:22:01 |     num_workers: 0\n",
      "18:22:01 |     nus: [0.7]\n",
      "18:22:01 |     optimizer: adamax\n",
      "18:22:01 |     output_scaling: 1.0\n",
      "18:22:01 |     override: \"{'task': 'blended_skill_talk', 'model_file': 'from_pretrained/model', 'num_examples': '2', 'skip_generation': False}\"\n",
      "18:22:01 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "18:22:01 |     person_tokens: False\n",
      "18:22:01 |     rank_candidates: False\n",
      "18:22:01 |     relu_dropout: 0.0\n",
      "18:22:01 |     save_after_valid: False\n",
      "18:22:01 |     save_every_n_secs: -1\n",
      "18:22:01 |     share_word_embeddings: True\n",
      "18:22:01 |     short_final_eval: False\n",
      "18:22:01 |     skip_generation: False\n",
      "18:22:01 |     special_tok_lst: None\n",
      "18:22:01 |     split_lines: False\n",
      "18:22:01 |     starttime: Feb28_17-23\n",
      "18:22:01 |     task: blended_skill_talk\n",
      "18:22:01 |     temperature: 1.0\n",
      "18:22:01 |     tensorboard_log: False\n",
      "18:22:01 |     tensorboard_logdir: None\n",
      "18:22:01 |     text_truncate: 512\n",
      "18:22:01 |     topk: 10\n",
      "18:22:01 |     topp: 0.9\n",
      "18:22:01 |     truncate: -1\n",
      "18:22:01 |     update_freq: 1\n",
      "18:22:01 |     use_reply: label\n",
      "18:22:01 |     validation_cutoff: 1.0\n",
      "18:22:01 |     validation_every_n_epochs: 0.25\n",
      "18:22:01 |     validation_every_n_secs: -1\n",
      "18:22:01 |     validation_every_n_steps: -1\n",
      "18:22:01 |     validation_max_exs: -1\n",
      "18:22:01 |     validation_metric: ppl\n",
      "18:22:01 |     validation_metric_mode: None\n",
      "18:22:01 |     validation_patience: 10\n",
      "18:22:01 |     validation_share_agent: False\n",
      "18:22:01 |     variant: xlm\n",
      "18:22:01 |     verbose: False\n",
      "18:22:01 |     wandb_entity: None\n",
      "18:22:01 |     wandb_log: False\n",
      "18:22:01 |     wandb_name: None\n",
      "18:22:01 |     wandb_project: None\n",
      "18:22:01 |     warmup_rate: 0.0001\n",
      "18:22:01 |     warmup_updates: 100\n",
      "18:22:01 |     weight_decay: None\n",
      "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk- - -\u001b[0;0m\n",
      "\u001b[0myour persona: i work as an electrician.\n",
      "your persona: i always sleep 8 hours a day.\n",
      "Electrician\n",
      "That sounds dangerous. Is it worth doing such a dangerous job?\n",
      "Wekk it is okay is you are well trained.  There are three levels: Apprentice, journeyman and Master.\n",
      "Which level are you at?\u001b[0;0m\n",
      "\u001b[1;94m    labels: I received on-the-job training when i first started\u001b[0;0m\n",
      "\u001b[0;95m     model: i am a apprentice . i am working 8 hours a day .\u001b[0;0m\n",
      "\u001b[0mThats great! How long have you been doing this work? \u001b[0;0m\n",
      "\u001b[1;94m    labels: For a good number of years now.\u001b[0;0m\n",
      "\u001b[0;95m     model: i have been doing this for about 10 years now .\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.display_model import DisplayModel\n",
    "DisplayModel.main(\n",
    "    task='blended_skill_talk',\n",
    "    model_file='from_pretrained/model',\n",
    "    num_examples=2,  skip_generation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f41323b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:23:56 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
      "18:23:56 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
      "18:23:56 | loading dictionary from from_pretrained/model.dict\n",
      "18:23:56 | num words = 54944\n",
      "18:23:56 | TransformerGenerator: full interactive mode on.\n",
      "18:23:57 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "18:23:57 | Loading existing model params from from_pretrained/model\n",
      "18:23:57 | Opt:\n",
      "18:23:57 |     activation: gelu\n",
      "18:23:57 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "18:23:57 |     adam_eps: 1e-08\n",
      "18:23:57 |     add_p1_after_newln: False\n",
      "18:23:57 |     aggregate_micro: False\n",
      "18:23:57 |     allow_missing_init_opts: False\n",
      "18:23:57 |     attention_dropout: 0.0\n",
      "18:23:57 |     batchsize: 24\n",
      "18:23:57 |     beam_block_full_context: True\n",
      "18:23:57 |     beam_block_list_filename: None\n",
      "18:23:57 |     beam_block_ngram: 3\n",
      "18:23:57 |     beam_context_block_ngram: 3\n",
      "18:23:57 |     beam_delay: 30\n",
      "18:23:57 |     beam_length_penalty: 0.65\n",
      "18:23:57 |     beam_min_length: 1\n",
      "18:23:57 |     beam_size: 1\n",
      "18:23:57 |     betas: '[0.9, 0.999]'\n",
      "18:23:57 |     bpe_add_prefix_space: None\n",
      "18:23:57 |     bpe_debug: False\n",
      "18:23:57 |     bpe_dropout: None\n",
      "18:23:57 |     bpe_merge: None\n",
      "18:23:57 |     bpe_vocab: None\n",
      "18:23:57 |     checkpoint_activations: False\n",
      "18:23:57 |     compute_tokenized_bleu: False\n",
      "18:23:57 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "18:23:57 |     datatype: train\n",
      "18:23:57 |     delimiter: '\\n'\n",
      "18:23:57 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "18:23:57 |     dict_endtoken: __end__\n",
      "18:23:57 |     dict_file: from_pretrained/model.dict\n",
      "18:23:57 |     dict_include_test: False\n",
      "18:23:57 |     dict_include_valid: False\n",
      "18:23:57 |     dict_initpath: None\n",
      "18:23:57 |     dict_language: english\n",
      "18:23:57 |     dict_loaded: True\n",
      "18:23:57 |     dict_lower: True\n",
      "18:23:57 |     dict_max_ngram_size: -1\n",
      "18:23:57 |     dict_maxexs: -1\n",
      "18:23:57 |     dict_maxtokens: -1\n",
      "18:23:57 |     dict_minfreq: 0\n",
      "18:23:57 |     dict_nulltoken: __null__\n",
      "18:23:57 |     dict_starttoken: __start__\n",
      "18:23:57 |     dict_textfields: text,labels\n",
      "18:23:57 |     dict_tokenizer: bpe\n",
      "18:23:57 |     dict_unktoken: __unk__\n",
      "18:23:57 |     display_add_fields: \n",
      "18:23:57 |     display_examples: False\n",
      "18:23:57 |     display_partner_persona: True\n",
      "18:23:57 |     display_prettify: False\n",
      "18:23:57 |     download_path: None\n",
      "18:23:57 |     dropout: 0.0\n",
      "18:23:57 |     dynamic_batching: full\n",
      "18:23:57 |     embedding_projection: random\n",
      "18:23:57 |     embedding_size: 512\n",
      "18:23:57 |     embedding_type: random\n",
      "18:23:57 |     embeddings_scale: True\n",
      "18:23:57 |     eval_batchsize: None\n",
      "18:23:57 |     eval_dynamic_batching: None\n",
      "18:23:57 |     evaltask: None\n",
      "18:23:57 |     ffn_size: 2048\n",
      "18:23:57 |     final_extra_opt: \n",
      "18:23:57 |     force_fp16_tokens: True\n",
      "18:23:57 |     fp16: True\n",
      "18:23:57 |     fp16_impl: safe\n",
      "18:23:57 |     gpu: -1\n",
      "18:23:57 |     gradient_clip: 0.1\n",
      "18:23:57 |     hide_labels: False\n",
      "18:23:57 |     history_add_global_end_token: None\n",
      "18:23:57 |     history_reversed: False\n",
      "18:23:57 |     history_size: -1\n",
      "18:23:57 |     image_cropsize: 224\n",
      "18:23:57 |     image_mode: raw\n",
      "18:23:57 |     image_size: 256\n",
      "18:23:57 |     include_initial_utterances: False\n",
      "18:23:57 |     include_personas: True\n",
      "18:23:57 |     inference: greedy\n",
      "18:23:57 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "18:23:57 |     init_opt: None\n",
      "18:23:57 |     interactive_mode: True\n",
      "18:23:57 |     interactive_task: True\n",
      "18:23:57 |     invsqrt_lr_decay_gamma: -1\n",
      "18:23:57 |     is_debug: False\n",
      "18:23:57 |     label_truncate: 128\n",
      "18:23:57 |     learn_positional_embeddings: True\n",
      "18:23:57 |     learningrate: 1e-05\n",
      "18:23:57 |     local_human_candidates_file: None\n",
      "18:23:57 |     log_every_n_secs: -1\n",
      "18:23:57 |     log_every_n_steps: 50\n",
      "18:23:57 |     log_keep_fields: all\n",
      "18:23:57 |     loglevel: info\n",
      "18:23:57 |     lr_scheduler: reduceonplateau\n",
      "18:23:57 |     lr_scheduler_decay: 0.5\n",
      "18:23:57 |     lr_scheduler_patience: 3\n",
      "18:23:57 |     max_train_steps: -1\n",
      "18:23:57 |     max_train_time: 3600.0\n",
      "18:23:57 |     metrics: default\n",
      "18:23:57 |     model: transformer/generator\n",
      "18:23:57 |     model_file: from_pretrained/model\n",
      "18:23:57 |     model_parallel: True\n",
      "18:23:57 |     momentum: 0\n",
      "18:23:57 |     multitask_weights: [1]\n",
      "18:23:57 |     mutators: None\n",
      "18:23:57 |     n_decoder_layers: -1\n",
      "18:23:57 |     n_encoder_layers: -1\n",
      "18:23:57 |     n_heads: 16\n",
      "18:23:57 |     n_layers: 8\n",
      "18:23:57 |     n_positions: 512\n",
      "18:23:57 |     n_segments: 0\n",
      "18:23:57 |     nesterov: True\n",
      "18:23:57 |     no_cuda: False\n",
      "18:23:57 |     num_epochs: -1\n",
      "18:23:57 |     num_workers: 0\n",
      "18:23:57 |     nus: [0.7]\n",
      "18:23:57 |     optimizer: adamax\n",
      "18:23:57 |     outfile: \n",
      "18:23:57 |     output_scaling: 1.0\n",
      "18:23:57 |     override: \"{'model_file': 'from_pretrained/model', 'task': 'blended_skill_talk', 'beam_block_ngram': 3, 'beam_context_block_ngram': 3}\"\n",
      "18:23:57 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "18:23:57 |     person_tokens: False\n",
      "18:23:57 |     rank_candidates: False\n",
      "18:23:57 |     relu_dropout: 0.0\n",
      "18:23:57 |     safe_personas_only: True\n",
      "18:23:57 |     save_after_valid: False\n",
      "18:23:57 |     save_every_n_secs: -1\n",
      "18:23:57 |     save_format: conversations\n",
      "18:23:57 |     share_word_embeddings: True\n",
      "18:23:57 |     short_final_eval: False\n",
      "18:23:57 |     single_turn: False\n",
      "18:23:57 |     skip_generation: True\n",
      "18:23:57 |     special_tok_lst: None\n",
      "18:23:57 |     split_lines: False\n",
      "18:23:57 |     starttime: Feb28_17-23\n",
      "18:23:57 |     task: blended_skill_talk\n",
      "18:23:57 |     temperature: 1.0\n",
      "18:23:57 |     tensorboard_log: False\n",
      "18:23:57 |     tensorboard_logdir: None\n",
      "18:23:57 |     text_truncate: 512\n",
      "18:23:57 |     topk: 10\n",
      "18:23:57 |     topp: 0.9\n",
      "18:23:57 |     truncate: -1\n",
      "18:23:57 |     update_freq: 1\n",
      "18:23:57 |     use_reply: label\n",
      "18:23:57 |     validation_cutoff: 1.0\n",
      "18:23:57 |     validation_every_n_epochs: 0.25\n",
      "18:23:57 |     validation_every_n_secs: -1\n",
      "18:23:57 |     validation_every_n_steps: -1\n",
      "18:23:57 |     validation_max_exs: -1\n",
      "18:23:57 |     validation_metric: ppl\n",
      "18:23:57 |     validation_metric_mode: None\n",
      "18:23:57 |     validation_patience: 10\n",
      "18:23:57 |     validation_share_agent: False\n",
      "18:23:57 |     variant: xlm\n",
      "18:23:57 |     verbose: False\n",
      "18:23:57 |     wandb_entity: None\n",
      "18:23:57 |     wandb_log: False\n",
      "18:23:57 |     wandb_name: None\n",
      "18:23:57 |     wandb_project: None\n",
      "18:23:57 |     warmup_rate: 0.0001\n",
      "18:23:57 |     warmup_updates: 100\n",
      "18:23:57 |     weight_decay: None\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "18:23:59 | creating task(s): blended_skill_talk\n",
      "[ loading personas.. ]\n",
      "\n",
      "  [NOTE: In the BST paper both partners have a persona.\n",
      "         You can choose to ignore yours, the model never sees it.\n",
      "         In the Blender paper, this was not used for humans.\n",
      "         You can also turn personas off with --include-personas False]\n",
      "\n",
      "\u001b[0;34m[context]:\u001b[0;0m \u001b[1myour persona: i love to shop.\n",
      "your persona: i'm in debt.\u001b[0;0m\n",
      "Enter Your Message: hello\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhello , how are you today ?\u001b[0;0m\n",
      "Enter Your Message: feeling scared\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhy are you scared ?\u001b[0;0m\n",
      "Enter Your Message: there is a war going on here\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1moh no ! i ' ve been looking for work in marketing for a while .\u001b[0;0m\n",
      "Enter Your Message: why\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' d like to spend more time with my friends and going country concerts\u001b[0;0m\n",
      "Enter Your Message: what will you do now\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' ll probably go to a concert . i love country music .\u001b[0;0m\n",
      "Enter Your Message: I can't do anything now\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat are you doing now ?\u001b[0;0m\n",
      "Enter Your Message: I am in a bunker\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1moh wow ! that must be scary .\u001b[0;0m\n",
      "Enter Your Message: yes\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat do you do for a living ?\u001b[0;0m\n",
      "Enter Your Message: I am a teacher\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat ' s great ! i am currently in college .\u001b[0;0m\n",
      "Enter Your Message: great\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat grade do you teach ?\u001b[0;0m\n",
      "Enter Your Message: high school\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat grade are you teaching ?\u001b[0;0m\n",
      "Enter Your Message: [EXIT]\n",
      "\n",
      "CHAT DONE.\n",
      "\n",
      "Your partner was playing the following persona:\n",
      "partner's persona: i'm recent college graduate looking for a job in marketing.\n",
      "partner's persona: i like spending time with friends playing sports and going to country concerts.\n"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.interactive import Interactive\n",
    "Interactive.main(model_file='from_pretrained/model', task='blended_skill_talk', beam_block_ngram= 3, beam_context_block_ngram= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400435e",
   "metadata": {},
   "source": [
    "## Train on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5477e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:19 | Opt:\n",
      "19:12:19 |     allow_missing_init_opts: False\n",
      "19:12:19 |     batchsize: 1\n",
      "19:12:19 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "19:12:19 |     datatype: train:ordered\n",
      "19:12:19 |     dict_class: None\n",
      "19:12:19 |     display_add_fields: \n",
      "19:12:19 |     download_path: None\n",
      "19:12:19 |     dynamic_batching: None\n",
      "19:12:19 |     hide_labels: False\n",
      "19:12:19 |     ignore_agent_reply: True\n",
      "19:12:19 |     image_cropsize: 224\n",
      "19:12:19 |     image_mode: raw\n",
      "19:12:19 |     image_size: 256\n",
      "19:12:19 |     init_model: None\n",
      "19:12:19 |     init_opt: None\n",
      "19:12:19 |     is_debug: False\n",
      "19:12:19 |     loglevel: info\n",
      "19:12:19 |     max_display_len: 1000\n",
      "19:12:19 |     model: None\n",
      "19:12:19 |     model_file: None\n",
      "19:12:19 |     multitask_weights: [1]\n",
      "19:12:19 |     mutators: None\n",
      "19:12:19 |     num_examples: 10\n",
      "19:12:19 |     override: \"{'task': 'my_teacher'}\"\n",
      "19:12:19 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "19:12:19 |     starttime: Feb28_19-12\n",
      "19:12:19 |     task: my_teacher\n",
      "19:12:19 |     verbose: False\n",
      "19:12:19 | creating task(s): my_teacher\n",
      " ~~ Loading from train.txt ~~ \n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
      "\u001b[0mtext:I am feeling really sad?   labels:why are you feeling sad?\u001b[0;0m\n",
      "   \u001b[1;94mtext:A war is going on here in Ukarin?   labels:yes, I know. Russia and Ukraine have had no formal diplomatic relations since 24 February 2022. The Russian Federation and Ukraine are currently in a state of war: the Russo-Ukrainian War began in 2014 following the Russian annexation of Crimea from Ukraine. In February 2022, Russia invaded Ukraine across a broad front.\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
      "\u001b[0mtext:Indeed. When will it end  labels:Hope it end soon\u001b[0;0m\n",
      "   \u001b[1;94mtext:I am feeling really scared   labels:Don't be scared. Be strong and be brave\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
      "\u001b[0mtext:How can I feel secured, feeling so insecured now   labels:I am here for you. Don't feel scared. We will get through it.\u001b[0;0m\n",
      "   \u001b[1;94mtext:I am in a bunker now   labels:That is scary. But things will be better soon. Don't lose hope.\u001b[0;0m\n",
      "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
      "\u001b[0mtext:thank you   labels:you are welcome. Hope everything works fine.\u001b[0;0m\n",
      "   \u001b[1;94mtext:bye   labels:bye.   episode_done=True\u001b[0;0m\n",
      "19:12:20 | epoch done\n",
      "19:12:20 | loaded 4 episodes with a total of 4 examples\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.teachers import register_teacher, DialogTeacher\n",
    "from parlai.scripts.display_model import DisplayModel\n",
    "from parlai.scripts.train_model import TrainModel\n",
    "from parlai.scripts.display_data import DisplayData\n",
    "from parlai.core.agents import register_agent, Agent\n",
    "file = open('data.txt')\n",
    "lines = file.readlines()\n",
    "\n",
    "odd_lines = []\n",
    "even_lines = []\n",
    "\n",
    "for i,line in enumerate(lines):\n",
    "    if i%2==0:\n",
    "        even_lines.append(line.strip('\\n'))\n",
    "    else:\n",
    "        odd_lines.append(line.strip('\\n'))\n",
    "from parlai.core.teachers import register_teacher, DialogTeacher\n",
    "!rm -rf out.txt\n",
    "\n",
    "@register_teacher(\"my_teacher\")\n",
    "class MyTeacher(DialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        # opt is the command line arguments.\n",
    "        \n",
    "        # What is this shared thing?\n",
    "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
    "        \n",
    "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
    "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
    "        # the fold name (train/valid/test) + \".txt\"\n",
    "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
    "        super().__init__(opt, shared)\n",
    "    \n",
    "    def setup_data(self, datafile):\n",
    "        # filename tells us where to load from.\n",
    "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
    "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
    "      \n",
    "        with open('out.txt','a') as out:\n",
    "\n",
    "          for e,o in zip(even_lines,odd_lines):\n",
    "            yield (e,o), True\n",
    "\n",
    "              #out.write(e+\",\" + o+'\\n')\n",
    "                \n",
    "           \n",
    "      \n",
    "        \n",
    "        \n",
    "DisplayData.main(task=\"my_teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79e5438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:14:11 | building dictionary first...\n",
      "19:14:11 | No model with opt yet at: from_custom_trained/model(.opt)\n",
      "19:14:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,is_debug: False,datapath: /usr/local/lib/python3.8/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: True,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,interactive_mode: False,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None\u001b[0m\n",
      "19:14:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --max-train-time -1 --save-every-n-secs 60.0 --save-after-valid True --validation-max-exs 20000 --validation-patience 15 --validation-metric-mode min --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
      "19:14:11 | loading dictionary from /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict\n",
      "19:14:11 | num words = 54944\n",
      "19:14:12 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "19:14:12 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "19:14:12 | Loading existing model params from /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "19:14:13 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
      "19:14:13 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
      "19:14:13 | Opt:\n",
      "19:14:13 |     activation: gelu\n",
      "19:14:13 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "19:14:13 |     adam_eps: 1e-08\n",
      "19:14:13 |     add_p1_after_newln: False\n",
      "19:14:13 |     aggregate_micro: False\n",
      "19:14:13 |     allow_missing_init_opts: False\n",
      "19:14:13 |     attention_dropout: 0.0\n",
      "19:14:13 |     batchsize: 24\n",
      "19:14:13 |     beam_block_full_context: True\n",
      "19:14:13 |     beam_block_list_filename: None\n",
      "19:14:13 |     beam_block_ngram: -1\n",
      "19:14:13 |     beam_context_block_ngram: -1\n",
      "19:14:13 |     beam_delay: 30\n",
      "19:14:13 |     beam_length_penalty: 0.65\n",
      "19:14:13 |     beam_min_length: 1\n",
      "19:14:13 |     beam_size: 1\n",
      "19:14:13 |     betas: '(0.9, 0.999)'\n",
      "19:14:13 |     bpe_add_prefix_space: None\n",
      "19:14:13 |     bpe_debug: False\n",
      "19:14:13 |     bpe_dropout: None\n",
      "19:14:13 |     bpe_merge: None\n",
      "19:14:13 |     bpe_vocab: None\n",
      "19:14:13 |     checkpoint_activations: False\n",
      "19:14:13 |     compute_tokenized_bleu: False\n",
      "19:14:13 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "19:14:13 |     datatype: train\n",
      "19:14:13 |     delimiter: '\\n'\n",
      "19:14:13 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "19:14:13 |     dict_endtoken: __end__\n",
      "19:14:13 |     dict_file: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict\n",
      "19:14:13 |     dict_include_test: False\n",
      "19:14:13 |     dict_include_valid: False\n",
      "19:14:13 |     dict_initpath: None\n",
      "19:14:13 |     dict_language: english\n",
      "19:14:13 |     dict_loaded: True\n",
      "19:14:13 |     dict_lower: True\n",
      "19:14:13 |     dict_max_ngram_size: -1\n",
      "19:14:13 |     dict_maxexs: -1\n",
      "19:14:13 |     dict_maxtokens: -1\n",
      "19:14:13 |     dict_minfreq: 0\n",
      "19:14:13 |     dict_nulltoken: __null__\n",
      "19:14:13 |     dict_starttoken: __start__\n",
      "19:14:13 |     dict_textfields: text,labels\n",
      "19:14:13 |     dict_tokenizer: bpe\n",
      "19:14:13 |     dict_unktoken: __unk__\n",
      "19:14:13 |     display_examples: False\n",
      "19:14:13 |     download_path: None\n",
      "19:14:13 |     dropout: 0.0\n",
      "19:14:13 |     dynamic_batching: full\n",
      "19:14:13 |     embedding_projection: random\n",
      "19:14:13 |     embedding_size: 512\n",
      "19:14:13 |     embedding_type: random\n",
      "19:14:13 |     embeddings_scale: True\n",
      "19:14:13 |     eval_batchsize: None\n",
      "19:14:13 |     eval_dynamic_batching: None\n",
      "19:14:13 |     evaltask: None\n",
      "19:14:13 |     ffn_size: 2048\n",
      "19:14:13 |     final_extra_opt: \n",
      "19:14:13 |     force_fp16_tokens: False\n",
      "19:14:13 |     fp16: True\n",
      "19:14:13 |     fp16_impl: safe\n",
      "19:14:13 |     gpu: -1\n",
      "19:14:13 |     gradient_clip: 0.1\n",
      "19:14:13 |     hide_labels: False\n",
      "19:14:13 |     history_add_global_end_token: None\n",
      "19:14:13 |     history_reversed: False\n",
      "19:14:13 |     history_size: -1\n",
      "19:14:13 |     image_cropsize: 224\n",
      "19:14:13 |     image_mode: raw\n",
      "19:14:13 |     image_size: 256\n",
      "19:14:13 |     inference: greedy\n",
      "19:14:13 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "19:14:13 |     init_opt: None\n",
      "19:14:13 |     interactive_mode: False\n",
      "19:14:13 |     invsqrt_lr_decay_gamma: -1\n",
      "19:14:13 |     is_debug: False\n",
      "19:14:13 |     label_truncate: 128\n",
      "19:14:13 |     learn_positional_embeddings: True\n",
      "19:14:13 |     learningrate: 1e-05\n",
      "19:14:13 |     load_from_checkpoint: True\n",
      "19:14:13 |     log_every_n_secs: -1\n",
      "19:14:13 |     log_every_n_steps: 50\n",
      "19:14:13 |     loglevel: info\n",
      "19:14:13 |     lr_scheduler: reduceonplateau\n",
      "19:14:13 |     lr_scheduler_decay: 0.5\n",
      "19:14:13 |     lr_scheduler_patience: 3\n",
      "19:14:13 |     max_train_steps: -1\n",
      "19:14:13 |     max_train_time: 600.0\n",
      "19:14:13 |     metrics: default\n",
      "19:14:13 |     model: transformer/generator\n",
      "19:14:13 |     model_file: from_custom_trained/model\n",
      "19:14:13 |     model_parallel: True\n",
      "19:14:13 |     momentum: 0\n",
      "19:14:13 |     multitask_weights: [1]\n",
      "19:14:13 |     mutators: None\n",
      "19:14:13 |     n_decoder_layers: -1\n",
      "19:14:13 |     n_encoder_layers: -1\n",
      "19:14:13 |     n_heads: 16\n",
      "19:14:13 |     n_layers: 8\n",
      "19:14:13 |     n_positions: 512\n",
      "19:14:13 |     n_segments: 0\n",
      "19:14:13 |     nesterov: True\n",
      "19:14:13 |     no_cuda: False\n",
      "19:14:13 |     num_epochs: -1\n",
      "19:14:13 |     num_workers: 0\n",
      "19:14:13 |     nus: (0.7,)\n",
      "19:14:13 |     optimizer: adamax\n",
      "19:14:13 |     output_scaling: 1.0\n",
      "19:14:13 |     override: \"{'task': 'my_teacher', 'model': 'transformer/generator', 'model_file': 'from_custom_trained/model', 'init_model': 'zoo:blender/blender_90M/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'model_parallel': True, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adamax', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 24, 'fp16': True, 'fp16_impl': 'safe', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
      "19:14:13 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "19:14:13 |     person_tokens: False\n",
      "19:14:13 |     rank_candidates: False\n",
      "19:14:13 |     relu_dropout: 0.0\n",
      "19:14:13 |     save_after_valid: False\n",
      "19:14:13 |     save_every_n_secs: -1\n",
      "19:14:13 |     share_word_embeddings: True\n",
      "19:14:13 |     short_final_eval: False\n",
      "19:14:13 |     skip_generation: True\n",
      "19:14:13 |     special_tok_lst: None\n",
      "19:14:13 |     split_lines: False\n",
      "19:14:13 |     starttime: Feb28_19-14\n",
      "19:14:13 |     task: my_teacher\n",
      "19:14:13 |     temperature: 1.0\n",
      "19:14:13 |     tensorboard_log: False\n",
      "19:14:13 |     tensorboard_logdir: None\n",
      "19:14:13 |     text_truncate: 512\n",
      "19:14:13 |     topk: 10\n",
      "19:14:13 |     topp: 0.9\n",
      "19:14:13 |     truncate: -1\n",
      "19:14:13 |     update_freq: 1\n",
      "19:14:13 |     use_reply: label\n",
      "19:14:13 |     validation_cutoff: 1.0\n",
      "19:14:13 |     validation_every_n_epochs: 0.25\n",
      "19:14:13 |     validation_every_n_secs: -1\n",
      "19:14:13 |     validation_every_n_steps: -1\n",
      "19:14:13 |     validation_max_exs: -1\n",
      "19:14:13 |     validation_metric: ppl\n",
      "19:14:13 |     validation_metric_mode: None\n",
      "19:14:13 |     validation_patience: 10\n",
      "19:14:13 |     validation_share_agent: False\n",
      "19:14:13 |     variant: xlm\n",
      "19:14:13 |     verbose: False\n",
      "19:14:13 |     wandb_entity: None\n",
      "19:14:13 |     wandb_log: False\n",
      "19:14:13 |     wandb_name: None\n",
      "19:14:13 |     wandb_project: None\n",
      "19:14:13 |     warmup_rate: 0.0001\n",
      "19:14:13 |     warmup_updates: 100\n",
      "19:14:13 |     weight_decay: None\n",
      "19:14:13 | creating task(s): my_teacher\n",
      " ~~ Loading from train.txt ~~ \n",
      "19:14:14 | training...\n",
      "19:14:17 | time:4s total_exs:212 total_steps:1 epochs:53.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2072       0          0 62.79  212  41.47    28 2.598 1.01e-07  5936  1758       0          0 13.44   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4286         0                    1 12932 3830 .2967\n",
      "\n",
      "19:14:17 | creating task(s): my_teacher\n",
      " ~~ Loading from valid.txt ~~ \n",
      "19:14:18 | running eval: valid\n",
      "19:14:18 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
      "19:14:18 | eval completed in 0.13s\n",
      "19:14:18 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 850.6       0          0 43.58    4 35.75 4.014 1.01e-07   143  1559       0          0 55.38      .3077   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    1  221 2410\n",
      "\u001b[0m\n",
      "19:14:18 | \u001b[1;32mnew best ppl: 55.38\u001b[0m\n",
      "19:14:18 | saving best valid model: from_custom_trained/model\n",
      "19:14:18 | Saving dictionary to from_custom_trained/model.dict\n",
      "19:14:25 | time:11s total_exs:372 total_steps:2 epochs:93.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   24.93     1  3988 748.6       0          0 30.03  160   22.2  52.7 3.887 2.01e-07  8432  1583       0          0 48.75   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3359         0                    2 12420 2331 .1879\n",
      "\n",
      "19:14:25 | running eval: valid\n",
      "19:14:25 | eval completed in 0.15s\n",
      "19:14:25 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 668.4       0          0 34.27    4 35.75 4.011 2.01e-07   143  1225       0          0 55.22      .3147   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    2  221 1894\n",
      "\u001b[0m\n",
      "19:14:25 | \u001b[1;32mnew best ppl: 55.22 (previous best was 55.38)\u001b[0m\n",
      "19:14:25 | saving best valid model: from_custom_trained/model\n",
      "19:14:34 | time:20s total_exs:756 total_steps:3 epochs:189.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "    14.1     1  5414  1521       0          0 107.9  384  39.45 20.31 3.458 3.01e-07  7798  2191       0          0 31.77   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2365         0                    3 13212 3712 .2816\n",
      "\n",
      "19:14:34 | running eval: valid\n",
      "19:14:34 | eval completed in 0.14s\n",
      "19:14:34 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 745.3       0          0 38.21    4 35.75 4.006 3.01e-07   143  1366       0          0 54.9      .3147   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    3  221 2112\n",
      "\u001b[0m\n",
      "19:14:34 | \u001b[1;32mnew best ppl: 54.9 (previous best was 55.22)\u001b[0m\n",
      "19:14:34 | saving best valid model: from_custom_trained/model\n",
      "19:14:41 | time:28s total_exs:1180 total_steps:4 epochs:295.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      15     1  6360  2235       0          0   149  424  66.01    14 5.812 4.01e-07  5936  2086       0          0 334.4   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2143         0                    4 12296 4320 .3525\n",
      "\n",
      "19:14:41 | running eval: valid\n",
      "19:14:42 | eval completed in 0.12s\n",
      "19:14:42 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 923.7       0          0 47.35    4 35.75 3.995 4.01e-07   143  1693       0          0 54.35      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    4  221 2617\n",
      "\u001b[0m\n",
      "19:14:42 | \u001b[1;32mnew best ppl: 54.35 (previous best was 54.9)\u001b[0m\n",
      "19:14:42 | saving best valid model: from_custom_trained/model\n",
      "19:14:48 | time:35s total_exs:1480 total_steps:5 epochs:370.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.37     1  4311  1751       0          0 121.9  300  37.23 18.41 3.984 5.01e-07  5523  2243       0          0 53.71   \n",
      "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
      "        .2314         0                    5 9834 3995 .4073\n",
      "\n",
      "19:14:48 | running eval: valid\n",
      "19:14:48 | eval completed in 0.11s\n",
      "19:14:48 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss       lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 964.3       0          0 49.37    4 35.75 3.989 5.01e-07   143  1767       0          0   54      .3217   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    5  221 2733\n",
      "\u001b[0m\n",
      "19:14:48 | \u001b[1;32mnew best ppl: 54 (previous best was 54.35)\u001b[0m\n",
      "19:14:48 | saving best valid model: from_custom_trained/model\n",
      "19:14:57 | time:43s total_exs:1640 total_steps:6 epochs:410.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 578.1       0          0 36.13  160  24.96    80 4.374 6.009e-07 12800  2891       0          0 79.37   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                    6 15360 3469 .2262\n",
      "\n",
      "19:14:57 | running eval: valid\n",
      "19:14:57 | eval completed in 0.11s\n",
      "19:14:57 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 973.2       0          0 49.86    4 35.75 3.982 6.009e-07   143  1784       0          0 53.64      .3217   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    6  221 2757\n",
      "\u001b[0m\n",
      "19:14:57 | \u001b[1;32mnew best ppl: 53.64 (previous best was 54)\u001b[0m\n",
      "19:14:57 | saving best valid model: from_custom_trained/model\n",
      "19:15:04 | time:50s total_exs:1852 total_steps:7 epochs:463.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2804       0          0 84.95  212  97.45    28 2.544 7.009e-07  5936  2379       0          0 12.73   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5000         0                    7 12932 5182 .4017\n",
      "\n",
      "19:15:04 | running eval: valid\n",
      "19:15:04 | eval completed in 0.11s\n",
      "19:15:04 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   986       0          0 50.54    4 35.75 3.973 7.009e-07   143  1807       0          0 53.12      .3217   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    7  221 2794\n",
      "\u001b[0m\n",
      "19:15:04 | \u001b[1;32mnew best ppl: 53.12 (previous best was 53.64)\u001b[0m\n",
      "19:15:04 | saving best valid model: from_custom_trained/model\n",
      "19:15:11 | time:58s total_exs:2064 total_steps:8 epochs:516.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2267       0          0 68.69  212  131.7    28 2.532 8.009e-07  5936  1923       0          0 12.58   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5000         0                    8 12932 4190 .3246\n",
      "\n",
      "19:15:11 | running eval: valid\n",
      "19:15:11 | eval completed in 0.11s\n",
      "19:15:11 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 953.3       0          0  48.8    4 35.75 3.964 8.009e-07   143  1747       0          0 52.65      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                    8  221 2701\n",
      "\u001b[0m\n",
      "19:15:11 | \u001b[1;32mnew best ppl: 52.65 (previous best was 53.12)\u001b[0m\n",
      "19:15:11 | saving best valid model: from_custom_trained/model\n",
      "19:15:20 | time:66s total_exs:2224 total_steps:9 epochs:556.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 576.2       0          0 36.01  160  24.85    80 4.354 9.009e-07 12800  2881       0          0 77.76   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                    9 15360 3457 .2253\n",
      "\n",
      "19:15:20 | running eval: valid\n",
      "19:15:21 | eval completed in 0.45s\n",
      "19:15:21 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 185.7       0          0 9.522    4 35.75 3.958 9.009e-07   143 340.5       0          0 52.33      .3147   \n",
      "    token_em  total_train_updates  tpb   tps  \n",
      "           0                    9  221 526.2\n",
      "\u001b[0m\n",
      "19:15:21 | \u001b[1;32mnew best ppl: 52.33 (previous best was 52.65)\u001b[0m\n",
      "19:15:21 | saving best valid model: from_custom_trained/model\n",
      "19:15:29 | time:75s total_exs:2608 total_steps:10 epochs:652.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.33     1  5501  1383       0          0 96.57  384   35.7 18.72 3.823 1.001e-06  7189  1808       0          0 45.75   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2323         0                   10 12690 3191 .2521\n",
      "\n",
      "19:15:29 | running eval: valid\n",
      "19:15:29 | eval completed in 0.12s\n",
      "19:15:29 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 909.3       0          0 46.59    4 35.75 3.946 1.001e-06   143  1667       0          0 51.73      .3217   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   10  221 2576\n",
      "\u001b[0m\n",
      "19:15:29 | \u001b[1;32mnew best ppl: 51.73 (previous best was 52.33)\u001b[0m\n",
      "19:15:29 | saving best valid model: from_custom_trained/model\n",
      "19:15:40 | time:86s total_exs:2768 total_steps:11 epochs:692.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 401.5       0          0 25.09  160  24.74    80 4.331 1.101e-06 12800  2007       0          0 76.03   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   11 15360 2409 .1570\n",
      "\n",
      "19:15:40 | running eval: valid\n",
      "19:15:40 | eval completed in 0.19s\n",
      "19:15:40 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 517.5       0          0 26.53    4 35.75 3.931 1.101e-06   143 948.6       0          0 50.96      .3217   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   11  221 1467\n",
      "\u001b[0m\n",
      "19:15:40 | \u001b[1;32mnew best ppl: 50.96 (previous best was 51.73)\u001b[0m\n",
      "19:15:40 | saving best valid model: from_custom_trained/model\n",
      "19:15:48 | time:94s total_exs:3152 total_steps:12 epochs:788.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.67     1  5634  1480       0          0 100.9  384  43.27  16.3 4.608 1.201e-06  6258  1644       0          0 100.3   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2244         0                   12 11892 3124 .2634\n",
      "\n",
      "19:15:48 | running eval: valid\n",
      "19:15:48 | eval completed in 0.31s\n",
      "19:15:48 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 289.3       0          0 14.84    4 35.75 3.914 1.201e-06   143 530.3       0          0 50.08      .3287   \n",
      "    token_em  total_train_updates  tpb   tps  \n",
      "           0                   12  221 819.6\n",
      "\u001b[0m\n",
      "19:15:48 | \u001b[1;32mnew best ppl: 50.08 (previous best was 50.96)\u001b[0m\n",
      "19:15:48 | saving best valid model: from_custom_trained/model\n",
      "19:15:57 | time:103s total_exs:3364 total_steps:13 epochs:841.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  1733       0          0 52.51  212  37.64    28  2.52 1.301e-06  5936  1470       0          0 12.43   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5357         0                   13 12932 3203 .2480\n",
      "\n",
      "19:15:57 | running eval: valid\n",
      "19:15:57 | eval completed in 0.18s\n",
      "19:15:57 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 543.2       0          0 27.85    4 35.75 3.896 1.301e-06   143 995.8       0          0 49.19      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   13  221 1540\n",
      "\u001b[0m\n",
      "19:15:57 | \u001b[1;32mnew best ppl: 49.19 (previous best was 50.08)\u001b[0m\n",
      "19:15:57 | saving best valid model: from_custom_trained/model\n",
      "19:16:06 | time:112s total_exs:3524 total_steps:14 epochs:881.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      16     1  2560 549.2       0          0 34.32  160   24.5    80 4.285 1.401e-06 12800  2746       0          0 72.6   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   14 15360 3295 .2147\n",
      "\n",
      "19:16:06 | running eval: valid\n",
      "19:16:06 | eval completed in 0.12s\n",
      "19:16:06 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 862.3       0          0  44.2    4 35.75 3.875 1.401e-06   143  1581       0          0 48.21      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   14  221 2445\n",
      "\u001b[0m\n",
      "19:16:06 | \u001b[1;32mnew best ppl: 48.21 (previous best was 49.19)\u001b[0m\n",
      "19:16:06 | saving best valid model: from_custom_trained/model\n",
      "19:16:15 | time:121s total_exs:3684 total_steps:15 epochs:921.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 569.2       0          0 35.58  160  24.39    80 4.265 1.501e-06 12800  2846       0          0 71.15   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   15 15360 3415 .2226\n",
      "\n",
      "19:16:15 | running eval: valid\n",
      "19:16:15 | eval completed in 0.12s\n",
      "19:16:15 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 905.2       0          0 46.39    4 35.75 3.853 1.501e-06   143  1659       0          0 47.13      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   15  221 2565\n",
      "\u001b[0m\n",
      "19:16:15 | \u001b[1;32mnew best ppl: 47.13 (previous best was 48.21)\u001b[0m\n",
      "19:16:15 | saving best valid model: from_custom_trained/model\n",
      "19:16:25 | time:131s total_exs:4068 total_steps:16 epochs:1017.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.59     1  5602  1022       0          0 70.04  384   39.3 16.88 4.283 1.601e-06  6482  1182       0          0 72.44   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2265         0                   16 12084 2204 .1827\n",
      "\n",
      "19:16:25 | running eval: valid\n",
      "19:16:25 | eval completed in 0.14s\n",
      "19:16:25 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 732.5       0          0 37.54    4 35.75 3.829 1.601e-06   143  1343       0          0 46.02      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   16  221 2075\n",
      "\u001b[0m\n",
      "19:16:25 | \u001b[1;32mnew best ppl: 46.02 (previous best was 47.13)\u001b[0m\n",
      "19:16:25 | saving best valid model: from_custom_trained/model\n",
      "19:16:34 | time:141s total_exs:4228 total_steps:17 epochs:1057.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 520.1       0          0  32.5  160  24.13    80 4.215 1.701e-06 12800  2600       0          0 67.72   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   17 15360 3121 .2045\n",
      "\n",
      "19:16:34 | running eval: valid\n",
      "19:16:35 | eval completed in 0.12s\n",
      "19:16:35 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 925.9       0          0 47.42    4 35.75 3.803 1.701e-06   143  1696       0          0 44.82      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   17  221 2624\n",
      "\u001b[0m\n",
      "19:16:35 | \u001b[1;32mnew best ppl: 44.82 (previous best was 46.02)\u001b[0m\n",
      "19:16:35 | saving best valid model: from_custom_trained/model\n",
      "19:16:42 | time:149s total_exs:4440 total_steps:18 epochs:1110.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2140       0          0 64.84  212  30.35    28 2.446 1.801e-06  5936  1815       0          0 11.54   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5357         0                   18 12932 3955 .3064\n",
      "\n",
      "19:16:42 | running eval: valid\n",
      "19:16:42 | eval completed in 0.13s\n",
      "19:16:42 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 837.6       0          0 42.93    4 35.75 3.776 1.801e-06   143  1535       0          0 43.65      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   18  221 2373\n",
      "\u001b[0m\n",
      "19:16:42 | \u001b[1;32mnew best ppl: 43.65 (previous best was 44.82)\u001b[0m\n",
      "19:16:42 | saving best valid model: from_custom_trained/model\n",
      "19:16:51 | time:157s total_exs:4600 total_steps:19 epochs:1150.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 564.8       0          0  35.3  160  23.83    80 4.159 1.901e-06 12800  2824       0          0 63.98   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   19 15360 3389 .2210\n",
      "\n",
      "19:16:51 | running eval: valid\n",
      "19:16:51 | eval completed in 0.12s\n",
      "19:16:51 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 886.3       0          0  45.4    4 35.75 3.747 1.901e-06   143  1624       0          0 42.4      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   19  221 2511\n",
      "\u001b[0m\n",
      "19:16:51 | \u001b[1;32mnew best ppl: 42.4 (previous best was 43.65)\u001b[0m\n",
      "19:16:51 | saving best valid model: from_custom_trained/model\n",
      "19:16:59 | time:165s total_exs:4984 total_steps:20 epochs:1246.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "    14.6     1  5605  1769       0          0 121.2  384  38.97 16.83 4.172 2.001e-06  6461  2039       0          0 64.86   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2263         0                   20 12066 3808 .3164\n",
      "\n",
      "19:16:59 | running eval: valid\n",
      "19:16:59 | eval completed in 0.13s\n",
      "19:16:59 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   826       0          0 42.34    4 35.75 3.717 2.001e-06   143  1514       0          0 41.14      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   20  221 2340\n",
      "\u001b[0m\n",
      "19:16:59 | \u001b[1;32mnew best ppl: 41.14 (previous best was 42.4)\u001b[0m\n",
      "19:16:59 | saving best valid model: from_custom_trained/model\n",
      "19:17:06 | time:172s total_exs:5196 total_steps:21 epochs:1299.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2471       0          0 74.87  212  29.39    28 2.387 2.101e-06  5936  2096       0          0 10.88   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5357         0                   21 12932 4567 .3538\n",
      "\n",
      "19:17:06 | running eval: valid\n",
      "19:17:06 | eval completed in 0.12s\n",
      "19:17:06 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 903.9       0          0 46.33    4 35.75 3.687 2.101e-06   143  1657       0          0 39.91      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   21  221 2563\n",
      "\u001b[0m\n",
      "19:17:06 | \u001b[1;32mnew best ppl: 39.91 (previous best was 41.14)\u001b[0m\n",
      "19:17:06 | saving best valid model: from_custom_trained/model\n",
      "19:17:15 | time:182s total_exs:5356 total_steps:22 epochs:1339.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 498.4       0          0 31.15  160  23.33    80 4.066 2.201e-06 12800  2492       0          0 58.32   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   22 15360 2990 .1956\n",
      "\n",
      "19:17:15 | running eval: valid\n",
      "19:17:16 | eval completed in 0.12s\n",
      "19:17:16 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 916.3       0          0 46.95    4 35.75 3.654 2.201e-06   143  1680       0          0 38.63      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   22  221 2596\n",
      "\u001b[0m\n",
      "19:17:16 | \u001b[1;32mnew best ppl: 38.63 (previous best was 39.91)\u001b[0m\n",
      "19:17:16 | saving best valid model: from_custom_trained/model\n",
      "19:17:23 | time:190s total_exs:5740 total_steps:23 epochs:1435.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.11     1  5418  1568       0          0 111.2  384  34.79 20.23 3.149 2.301e-06  7770  2249       0          0 23.31   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2363         0                   23 13188 3818 .2902\n",
      "\n",
      "19:17:23 | running eval: valid\n",
      "19:17:24 | eval completed in 0.11s\n",
      "19:17:24 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 935.8       0          0 47.96    4 35.75 3.622 2.301e-06   143  1715       0          0 37.4      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   23  221 2651\n",
      "\u001b[0m\n",
      "19:17:24 | \u001b[1;32mnew best ppl: 37.4 (previous best was 38.63)\u001b[0m\n",
      "19:17:24 | saving best valid model: from_custom_trained/model\n",
      "19:17:31 | time:198s total_exs:6084 total_steps:24 epochs:1521.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.75     1  5074  1443       0          0  97.8  344  45.17 15.75 4.375 2.401e-06  5418  1540       0          0 79.42   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2222         0                   24 10492 2983 .2849\n",
      "\n",
      "19:17:31 | running eval: valid\n",
      "19:17:32 | eval completed in 0.15s\n",
      "19:17:32 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 678.6       0          0 34.79    4 35.75 3.589 2.401e-06   143  1244       0          0 36.18      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   24  221 1923\n",
      "\u001b[0m\n",
      "19:17:32 | \u001b[1;32mnew best ppl: 36.18 (previous best was 37.4)\u001b[0m\n",
      "19:17:32 | saving best valid model: from_custom_trained/model\n",
      "19:17:41 | time:207s total_exs:6244 total_steps:25 epochs:1561.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 589.1       0          0 36.82  160  22.78    80  3.97 2.501e-06 12800  2946       0          0 52.97   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3000         0                   25 15360 3535 .2319\n",
      "\n",
      "19:17:41 | running eval: valid\n",
      "19:17:41 | eval completed in 0.12s\n",
      "19:17:41 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 935.1       0          0 47.92    4 35.75 3.553 2.501e-06   143  1714       0          0 34.93      .3287   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   25  221 2652\n",
      "\u001b[0m\n",
      "19:17:41 | \u001b[1;32mnew best ppl: 34.93 (previous best was 36.18)\u001b[0m\n",
      "19:17:41 | saving best valid model: from_custom_trained/model\n",
      "19:17:48 | time:215s total_exs:6456 total_steps:26 epochs:1614.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      33     1  6996  2220       0          0 67.27  212  28.06    28 2.264 2.601e-06  5936  1884       0          0 9.62   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5357         0                   26 12932 4104 .3179\n",
      "\n",
      "19:17:48 | running eval: valid\n",
      "19:17:48 | eval completed in 0.13s\n",
      "19:17:48 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 822.3       0          0 42.14    4 35.75 3.518 2.601e-06   143  1507       0          0 33.73      .3497   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   26  221 2330\n",
      "\u001b[0m\n",
      "19:17:48 | \u001b[1;32mnew best ppl: 33.73 (previous best was 34.93)\u001b[0m\n",
      "19:17:48 | saving best valid model: from_custom_trained/model\n",
      "19:17:57 | time:223s total_exs:6668 total_steps:27 epochs:1667.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  1771       0          0 53.66  212  27.76    28 2.234 2.701e-06  5936  1503       0          0 9.337   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5357         0                   27 12932 3273 .2535\n",
      "\n",
      "19:17:57 | running eval: valid\n",
      "19:17:57 | eval completed in 0.18s\n",
      "19:17:57 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 557.5       0          0 28.58    4 35.75 3.484 2.701e-06   143  1022       0          0 32.58      .3566   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   27  221 1580\n",
      "\u001b[0m\n",
      "19:17:57 | \u001b[1;32mnew best ppl: 32.58 (previous best was 33.73)\u001b[0m\n",
      "19:17:57 | saving best valid model: from_custom_trained/model\n",
      "19:18:05 | time:232s total_exs:7052 total_steps:28 epochs:1763.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.43     1  5543  1305       0          0  90.4  384  32.99 17.96  3.51 2.801e-06  6895  1623       0          0 33.46   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2300         0                   28 12438 2928 .2359\n",
      "\n",
      "19:18:05 | running eval: valid\n",
      "19:18:06 | eval completed in 0.13s\n",
      "19:18:06 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 829.8       0          0 42.53    4 35.75 3.448 2.801e-06   143  1521       0          0 31.45      .3636   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   28  221 2351\n",
      "\u001b[0m\n",
      "19:18:06 | \u001b[1;32mnew best ppl: 31.45 (previous best was 32.58)\u001b[0m\n",
      "19:18:06 | saving best valid model: from_custom_trained/model\n",
      "19:18:15 | time:241s total_exs:7212 total_steps:29 epochs:1803.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 522.6       0          0 32.66  160  21.98    80 3.842 2.901e-06 12800  2613       0          0 46.61   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3500         0                   29 15360 3135 .2057\n",
      "\n",
      "19:18:15 | running eval: valid\n",
      "19:18:15 | eval completed in 0.12s\n",
      "19:18:15 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 880.8       0          0 45.15    4 35.75 3.411 2.901e-06   143  1615       0          0 30.3      .3846   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   29  221 2496\n",
      "\u001b[0m\n",
      "19:18:15 | \u001b[1;32mnew best ppl: 30.3 (previous best was 31.45)\u001b[0m\n",
      "19:18:15 | saving best valid model: from_custom_trained/model\n",
      "19:18:22 | time:248s total_exs:7536 total_steps:30 epochs:1884.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.53     1  4708  1735       0          0 119.4  324   34.1 17.28 3.589 3.001e-06  5600  2063       0          0 36.19   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2857         0                   30 10308 3798 .3694\n",
      "\n",
      "19:18:22 | running eval: valid\n",
      "19:18:22 | eval completed in 0.12s\n",
      "19:18:22 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 936.4       0          0 47.99    4 35.75 3.373 3.001e-06   143  1717       0          0 29.18      .4056   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   30  221 2653\n",
      "\u001b[0m\n",
      "19:18:22 | \u001b[1;32mnew best ppl: 29.18 (previous best was 30.3)\u001b[0m\n",
      "19:18:22 | saving best valid model: from_custom_trained/model\n",
      "19:18:31 | time:257s total_exs:7696 total_steps:31 epochs:1924.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      16     1  2560 562.6       0          0 35.16  160  21.53    80 3.775 3.101e-06 12800  2813       0          0 43.6   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3750         0                   31 15360 3376 .2216\n",
      "\n",
      "19:18:31 | running eval: valid\n",
      "19:18:31 | eval completed in 0.12s\n",
      "19:18:31 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 930.7       0          0  47.7    4 35.75 3.334 3.101e-06   143  1706       0          0 28.05      .4056   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   31  221 2639\n",
      "\u001b[0m\n",
      "19:18:31 | \u001b[1;32mnew best ppl: 28.05 (previous best was 29.18)\u001b[0m\n",
      "19:18:31 | saving best valid model: from_custom_trained/model\n",
      "19:18:38 | time:264s total_exs:7908 total_steps:32 epochs:1977.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2756       0          0 83.53  212   25.9    28  2.07 3.201e-06  5936  2339       0          0 7.927   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .6429         0                   32 12932 5095 .3948\n",
      "\n",
      "19:18:38 | running eval: valid\n",
      "19:18:38 | eval completed in 0.12s\n",
      "19:18:38 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 929.4       0          0 47.63    4 35.75 3.296 3.201e-06   143  1704       0          0   27      .4126   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   32  221 2635\n",
      "\u001b[0m\n",
      "19:18:38 | \u001b[1;32mnew best ppl: 27 (previous best was 28.05)\u001b[0m\n",
      "19:18:38 | saving best valid model: from_custom_trained/model\n",
      "19:18:47 | time:273s total_exs:8068 total_steps:33 epochs:2017.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 537.8       0          0 33.61  160  21.04    80 3.702 3.301e-06 12800  2689       0          0 40.53   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3875         0                   33 15360 3227 .2103\n",
      "\n",
      "19:18:47 | running eval: valid\n",
      "19:18:47 | eval completed in 0.12s\n",
      "19:18:47 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 935.4       0          0 47.95    4 35.75 3.256 3.301e-06   143  1715       0          0 25.95      .4126   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   33  221 2652\n",
      "\u001b[0m\n",
      "19:18:47 | \u001b[1;32mnew best ppl: 25.95 (previous best was 27)\u001b[0m\n",
      "19:18:47 | saving best valid model: from_custom_trained/model\n",
      "19:18:56 | time:283s total_exs:8228 total_steps:34 epochs:2057.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560   505       0          0 31.56  160  20.76    80 3.661 3.401e-06 12800  2525       0          0 38.91   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3875         0                   34 15360 3030 .1976\n",
      "\n",
      "19:18:56 | running eval: valid\n",
      "19:18:56 | eval completed in 0.13s\n",
      "19:18:56 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 788.6       0          0 40.42    4 35.75 3.215 3.401e-06   143  1446       0          0 24.9      .4126   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   34  221 2236\n",
      "\u001b[0m\n",
      "19:18:56 | \u001b[1;32mnew best ppl: 24.9 (previous best was 25.95)\u001b[0m\n",
      "19:18:56 | saving best valid model: from_custom_trained/model\n",
      "19:19:05 | time:291s total_exs:8612 total_steps:35 epochs:2153.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "    14.7     1  5644  1426       0          0 97.03  384  37.14 16.11 3.635 3.501e-06  6188  1564       0          0 37.88   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2857         0                   35 11832 2990 .2532\n",
      "\n",
      "19:19:05 | running eval: valid\n",
      "19:19:05 | eval completed in 0.18s\n",
      "19:19:05 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 728.3       0          0  37.3    4 35.75 3.175 3.501e-06   143  1334       0          0 23.92      .4126   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   35  221 2063\n",
      "\u001b[0m\n",
      "19:19:05 | \u001b[1;32mnew best ppl: 23.92 (previous best was 24.9)\u001b[0m\n",
      "19:19:05 | saving best valid model: from_custom_trained/model\n",
      "19:19:12 | time:298s total_exs:8824 total_steps:36 epochs:2206.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2678       0          0 81.14  212  24.26    28 1.948 3.601e-06  5936  2272       0          0 7.014   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .6429         0                   36 12932 4950 .3835\n",
      "\n",
      "19:19:12 | running eval: valid\n",
      "19:19:12 | eval completed in 0.14s\n",
      "19:19:12 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   725       0          0 37.16    4 35.75 3.136 3.601e-06   143  1329       0          0 23.01      .4126   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   36  221 2054\n",
      "\u001b[0m\n",
      "19:19:12 | \u001b[1;32mnew best ppl: 23.01 (previous best was 23.92)\u001b[0m\n",
      "19:19:12 | saving best valid model: from_custom_trained/model\n",
      "19:19:19 | time:305s total_exs:9036 total_steps:37 epochs:2259.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2761       0          0 83.67  212  23.85    28 1.914 3.701e-06  5936  2343       0          0 6.782   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .6429         0                   37 12932 5104 .3955\n",
      "\n",
      "19:19:19 | running eval: valid\n",
      "19:19:19 | eval completed in 0.12s\n",
      "19:19:19 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 922.6       0          0 47.29    4 35.75 3.099 3.701e-06   143  1691       0          0 22.18      .4196   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   37  221 2614\n",
      "\u001b[0m\n",
      "19:19:19 | \u001b[1;32mnew best ppl: 22.18 (previous best was 23.01)\u001b[0m\n",
      "19:19:19 | saving best valid model: from_custom_trained/model\n",
      "19:19:27 | time:314s total_exs:9196 total_steps:38 epochs:2299.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 585.4       0          0 36.59  160   19.7    80 3.502 3.801e-06 12800  2927       0          0 33.18   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3875         0                   38 15360 3513 .2289\n",
      "\n",
      "19:19:27 | running eval: valid\n",
      "19:19:28 | eval completed in 0.12s\n",
      "19:19:28 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 944.3       0          0 46.86    4 35.75 3.061 3.801e-06   143  1676       0          0 21.35      .4266   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   38  221 2676\n",
      "\u001b[0m\n",
      "19:19:28 | \u001b[1;32mnew best ppl: 21.35 (previous best was 22.18)\u001b[0m\n",
      "19:19:28 | saving best valid model: from_custom_trained/model\n",
      "19:19:35 | time:321s total_exs:9580 total_steps:39 epochs:2395.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.09     1  5411  1879       0          0 133.4  384   31.4 20.36 2.624 3.901e-06  7819  2716       0          0 13.78   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .2902         0                   39 13230 4595 .3483\n",
      "\n",
      "19:19:35 | running eval: valid\n",
      "19:19:35 | eval completed in 0.12s\n",
      "19:19:35 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 940.2       0          0 48.18    4 35.75 3.025 3.901e-06   143  1723       0          0 20.59      .4266   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   39  221 2664\n",
      "\u001b[0m\n",
      "19:19:35 | \u001b[1;32mnew best ppl: 20.59 (previous best was 21.35)\u001b[0m\n",
      "19:19:35 | saving best valid model: from_custom_trained/model\n",
      "19:19:42 | time:329s total_exs:9792 total_steps:40 epochs:2448.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2305       0          0 69.84  212  22.66    28 1.805 4.001e-06  5936  1956       0          0 6.082   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .6786         0                   40 12932 4260 .3307\n",
      "\n",
      "19:19:42 | running eval: valid\n",
      "19:19:42 | eval completed in 0.12s\n",
      "19:19:42 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 908.8       0          0 46.58    4 35.75  2.99 4.001e-06   143  1666       0          0 19.88      .4336   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   40  221 2575\n",
      "\u001b[0m\n",
      "19:19:42 | \u001b[1;32mnew best ppl: 19.88 (previous best was 20.59)\u001b[0m\n",
      "19:19:42 | saving best valid model: from_custom_trained/model\n",
      "19:19:52 | time:338s total_exs:9952 total_steps:41 epochs:2488.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   23.54     1  3767 753.3       0          0    32  160  15.87 56.92 3.039 4.101e-06  9108  1821       0          0 20.88   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4510         0                   41 12875 2575 .2002\n",
      "\n",
      "19:19:52 | running eval: valid\n",
      "19:19:52 | eval completed in 0.12s\n",
      "19:19:52 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 891.1       0          0 45.67    4 35.75 2.953 4.101e-06   143  1633       0          0 19.16      .4336   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   41  221 2526\n",
      "\u001b[0m\n",
      "19:19:52 | \u001b[1;32mnew best ppl: 19.16 (previous best was 19.88)\u001b[0m\n",
      "19:19:52 | saving best valid model: from_custom_trained/model\n",
      "19:19:59 | time:345s total_exs:10376 total_steps:42 epochs:2594.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      15     1  6360  2266       0          0 151.1  424  49.49    14 3.868 4.201e-06  5936  2115       0          0 47.87   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3571         0                   42 12296 4382 .3574\n",
      "\n",
      "19:19:59 | running eval: valid\n",
      "19:19:59 | eval completed in 0.12s\n",
      "19:19:59 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 933.7       0          0 47.86    4 35.75 2.918 4.201e-06   143  1712       0          0 18.5      .4615   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   42  221 2645\n",
      "\u001b[0m\n",
      "19:19:59 | \u001b[1;32mnew best ppl: 18.5 (previous best was 19.16)\u001b[0m\n",
      "19:19:59 | saving best valid model: from_custom_trained/model\n",
      "19:20:07 | time:353s total_exs:10536 total_steps:43 epochs:2634.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 705.9       0          0 44.12  160  18.75    80 3.323 4.301e-06 12800  3530       0          0 27.74   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4125         0                   43 15360 4236 .2809\n",
      "\n",
      "19:20:07 | running eval: valid\n",
      "19:20:07 | eval completed in 0.11s\n",
      "19:20:07 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 975.1       0          0 49.97    4 35.75 2.881 4.301e-06   143  1787       0          0 17.83      .4615   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   43  221 2763\n",
      "\u001b[0m\n",
      "19:20:07 | \u001b[1;32mnew best ppl: 17.83 (previous best was 18.5)\u001b[0m\n",
      "19:20:07 | saving best valid model: from_custom_trained/model\n",
      "19:20:15 | time:361s total_exs:10696 total_steps:44 epochs:2674.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 694.7       0          0 43.42  160  18.59    80 3.283 4.401e-06 12800  3474       0          0 26.66   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4125         0                   44 15360 4168 .2741\n",
      "\n",
      "19:20:15 | running eval: valid\n",
      "19:20:15 | eval completed in 0.11s\n",
      "19:20:15 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78  1011       0          0 51.82    4 35.75 2.842 4.401e-06   143  1854       0          0 17.15      .4615   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   44  221 2865\n",
      "\u001b[0m\n",
      "19:20:15 | \u001b[1;32mnew best ppl: 17.15 (previous best was 17.83)\u001b[0m\n",
      "19:20:15 | saving best valid model: from_custom_trained/model\n",
      "19:20:22 | time:368s total_exs:11080 total_steps:45 epochs:2770.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.47     1  5555  2293       0          0 158.5  384  27.33 17.74 2.858 4.501e-06  6811  2812       0          0 17.44   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3684         0                   45 12366 5105 .4142\n",
      "\n",
      "19:20:22 | running eval: valid\n",
      "19:20:22 | eval completed in 0.11s\n",
      "19:20:22 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78  1001       0          0  51.3    4 35.75 2.803 4.501e-06   143  1835       0          0 16.5      .4615   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   45  221 2836\n",
      "\u001b[0m\n",
      "19:20:22 | \u001b[1;32mnew best ppl: 16.5 (previous best was 17.15)\u001b[0m\n",
      "19:20:22 | saving best valid model: from_custom_trained/model\n",
      "19:20:29 | time:376s total_exs:11460 total_steps:46 epochs:2865.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.27     1  5422  1820       0          0 127.6  380  27.94 19.12 2.593 4.601e-06  7266  2439       0          0 13.37   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .3521         0                   46 12688 4259 .3367\n",
      "\n",
      "19:20:29 | running eval: valid\n",
      "19:20:29 | eval completed in 0.12s\n",
      "19:20:29 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 930.7       0          0  47.7    4 35.75 2.765 4.601e-06   143  1706       0          0 15.89      .4685   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   46  221 2637\n",
      "\u001b[0m\n",
      "19:20:29 | \u001b[1;32mnew best ppl: 15.89 (previous best was 16.5)\u001b[0m\n",
      "19:20:29 | saving best valid model: from_custom_trained/model\n",
      "19:20:38 | time:384s total_exs:11620 total_steps:47 epochs:2905.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 596.9       0          0 37.31  160  18.13    80 3.159 4.701e-06 12800  2985       0          0 23.54   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4125         0                   47 15360 3581 .2352\n",
      "\n",
      "19:20:38 | running eval: valid\n",
      "19:20:38 | eval completed in 0.12s\n",
      "19:20:38 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 851.5       0          0 43.61    4 35.75 2.726 4.701e-06   143  1560       0          0 15.27      .4895   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   47  221 2413\n",
      "\u001b[0m\n",
      "19:20:38 | \u001b[1;32mnew best ppl: 15.27 (previous best was 15.89)\u001b[0m\n",
      "19:20:38 | saving best valid model: from_custom_trained/model\n",
      "19:20:45 | time:391s total_exs:11832 total_steps:48 epochs:2958.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2700       0          0 81.83  212  20.47    28 1.574 4.801e-06  5936  2291       0          0 4.826   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7143         0                   48 12932 4992 .3867\n",
      "\n",
      "19:20:45 | running eval: valid\n",
      "19:20:45 | eval completed in 0.13s\n",
      "19:20:45 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 811.9       0          0 41.62    4 35.75 2.688 4.801e-06   143  1488       0          0 14.7      .4965   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   48  221 2300\n",
      "\u001b[0m\n",
      "19:20:45 | \u001b[1;32mnew best ppl: 14.7 (previous best was 15.27)\u001b[0m\n",
      "19:20:45 | saving best valid model: from_custom_trained/model\n",
      "19:20:54 | time:400s total_exs:11992 total_steps:49 epochs:2998.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 554.4       0          0 34.65  160  17.87    80 3.075 4.901e-06 12800  2772       0          0 21.65   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4250         0                   49 15360 3327 .2168\n",
      "\n",
      "19:20:54 | running eval: valid\n",
      "19:20:54 | eval completed in 0.13s\n",
      "19:20:54 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 801.3       0          0 41.07    4 35.75 2.648 4.901e-06   143  1469       0          0 14.12      .4965   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   49  221 2272\n",
      "\u001b[0m\n",
      "19:20:54 | \u001b[1;32mnew best ppl: 14.12 (previous best was 14.7)\u001b[0m\n",
      "19:20:54 | saving best valid model: from_custom_trained/model\n",
      "19:21:03 | time:409s total_exs:12152 total_steps:50 epochs:3038.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 586.6       0          0 36.66  160  17.73    80 3.029 5.001e-06 12800  2933       0          0 20.68   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4250         0                   50 15360 3520 .2294\n",
      "\n",
      "19:21:03 | running eval: valid\n",
      "19:21:03 | eval completed in 0.13s\n",
      "19:21:03 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 850.4       0          0 43.59    4 35.75 2.606 5.001e-06   143  1559       0          0 13.54      .4965   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   50  221 2409\n",
      "\u001b[0m\n",
      "19:21:03 | \u001b[1;32mnew best ppl: 13.54 (previous best was 14.12)\u001b[0m\n",
      "19:21:03 | saving best valid model: from_custom_trained/model\n",
      "19:21:10 | time:416s total_exs:12364 total_steps:51 epochs:3091.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2300       0          0 69.68  212  19.87    28   1.5 5.1e-06  5936  1951       0          0 4.484   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7500         0                   51 12932 4251 .3336\n",
      "\n",
      "19:21:10 | running eval: valid\n",
      "19:21:10 | eval completed in 0.13s\n",
      "19:21:10 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 809.2       0          0 41.48    4 35.75 2.566 5.1e-06   143  1483       0          0 13.01      .4965   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   51  221 2360\n",
      "\u001b[0m\n",
      "19:21:10 | \u001b[1;32mnew best ppl: 13.01 (previous best was 13.54)\u001b[0m\n",
      "19:21:10 | saving best valid model: from_custom_trained/model\n",
      "19:21:17 | time:424s total_exs:12748 total_steps:52 epochs:3187.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.18     1  5447  1797       0          0 126.7  384  28.47 19.71 2.289 5.2e-06  7567  2496       0          0 9.861   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4699         0                   52 13014 4293 .3308\n",
      "\n",
      "19:21:17 | running eval: valid\n",
      "19:21:18 | eval completed in 0.12s\n",
      "19:21:18 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 929.2       0          0 47.63    4 35.75 2.526 5.2e-06   143  1703       0          0 12.51      .4895   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   52  221 2633\n",
      "\u001b[0m\n",
      "19:21:18 | \u001b[1;32mnew best ppl: 12.51 (previous best was 13.01)\u001b[0m\n",
      "19:21:18 | saving best valid model: from_custom_trained/model\n",
      "19:21:25 | time:431s total_exs:13132 total_steps:53 epochs:3283.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.92     1  5729  2049       0          0 137.3  384  39.89 14.57 3.147 5.3e-06  5593  2000       0          0 23.27   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4341         0                   53 11322 4049 .3586\n",
      "\n",
      "19:21:25 | running eval: valid\n",
      "19:21:25 | eval completed in 0.12s\n",
      "19:21:25 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 895.8       0          0 45.91    4 35.75 2.488 5.3e-06   143  1642       0          0 12.04      .4895   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   53  221 2538\n",
      "\u001b[0m\n",
      "19:21:25 | \u001b[1;32mnew best ppl: 12.04 (previous best was 12.51)\u001b[0m\n",
      "19:21:25 | saving best valid model: from_custom_trained/model\n",
      "19:21:33 | time:440s total_exs:13292 total_steps:54 epochs:3323.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 607.2       0          0 37.95  160  17.23    80 2.849 5.4e-06 12800  3036       0          0 17.26   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4125         0                   54 15360 3643 .2374\n",
      "\n",
      "19:21:33 | running eval: valid\n",
      "19:21:33 | eval completed in 0.12s\n",
      "19:21:33 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 920.2       0          0 47.16    4 35.75 2.447 5.4e-06   143  1687       0          0 11.56      .5105   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   54  221 2609\n",
      "\u001b[0m\n",
      "19:21:33 | \u001b[1;32mnew best ppl: 11.56 (previous best was 12.04)\u001b[0m\n",
      "19:21:33 | saving best valid model: from_custom_trained/model\n",
      "19:21:42 | time:448s total_exs:13452 total_steps:55 epochs:3363.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 612.6       0          0 38.29  160  17.12    80 2.804 5.5e-06 12800  3063       0          0 16.51   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4250         0                   55 15360 3676 .2395\n",
      "\n",
      "19:21:42 | running eval: valid\n",
      "19:21:42 | eval completed in 0.12s\n",
      "19:21:42 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   911       0          0 46.69    4 35.75 2.405 5.5e-06   143  1670       0          0 11.08      .5245   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   55  221 2583\n",
      "\u001b[0m\n",
      "19:21:42 | \u001b[1;32mnew best ppl: 11.08 (previous best was 11.56)\u001b[0m\n",
      "19:21:42 | saving best valid model: from_custom_trained/model\n",
      "19:21:49 | time:456s total_exs:13836 total_steps:56 epochs:3459.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.47     1  5558  1941       0          0 134.1  384  25.74 17.68 2.403 5.6e-06  6790  2371       0          0 11.06   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5387         0                   56 12348 4312 .3502\n",
      "\n",
      "19:21:49 | running eval: valid\n",
      "19:21:49 | eval completed in 0.11s\n",
      "19:21:49 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 954.8       0          0 48.93    4 35.75 2.362 5.6e-06   143  1750       0          0 10.61      .5385   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   56  221 2705\n",
      "\u001b[0m\n",
      "19:21:49 | \u001b[1;32mnew best ppl: 10.61 (previous best was 11.08)\u001b[0m\n",
      "19:21:49 | saving best valid model: from_custom_trained/model\n",
      "19:21:56 | time:462s total_exs:14048 total_steps:57 epochs:3512.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2994       0          0 90.73  212   18.9    28 1.362 5.7e-06  5936  2541       0          0 3.903   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7500         0                   57 12932 5535 .4311\n",
      "\n",
      "19:21:56 | running eval: valid\n",
      "19:21:56 | eval completed in 0.11s\n",
      "19:21:56 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 951.4       0          0 48.76    4 35.75 2.321 5.7e-06   143  1744       0          0 10.19      .5455   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   57  221 2696\n",
      "\u001b[0m\n",
      "19:21:56 | \u001b[1;32mnew best ppl: 10.19 (previous best was 10.61)\u001b[0m\n",
      "19:21:56 | saving best valid model: from_custom_trained/model\n",
      "19:22:05 | time:471s total_exs:14208 total_steps:58 epochs:3552.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560   608       0          0    38  160   16.8    80 2.664 5.8e-06 12800  3040       0          0 14.35   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4625         0                   58 15360 3648 .2377\n",
      "\n",
      "19:22:05 | running eval: valid\n",
      "19:22:05 | eval completed in 0.11s\n",
      "19:22:05 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 953.8       0          0 48.82    4 35.75 2.278 5.8e-06   143  1746       0          0 9.757      .5455   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   58  221 2703\n",
      "\u001b[0m\n",
      "19:22:05 | \u001b[1;32mnew best ppl: 9.757 (previous best was 10.19)\u001b[0m\n",
      "19:22:05 | saving best valid model: from_custom_trained/model\n",
      "19:22:11 | time:478s total_exs:14420 total_steps:59 epochs:3605.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2769       0          0  83.9  212   18.6    28 1.312 5.9e-06  5936  2349       0          0 3.715   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7500         0                   59 12932 5118 .3966\n",
      "\n",
      "19:22:11 | running eval: valid\n",
      "19:22:12 | eval completed in 0.11s\n",
      "19:22:12 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   958       0          0  49.1    4 35.75 2.237 5.9e-06   143  1756       0          0 9.364      .5455   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   59  221 2714\n",
      "\u001b[0m\n",
      "19:22:12 | \u001b[1;32mnew best ppl: 9.364 (previous best was 9.757)\u001b[0m\n",
      "19:22:12 | saving best valid model: from_custom_trained/model\n",
      "19:22:19 | time:485s total_exs:14804 total_steps:60 epochs:3701.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.63     1  5617  2021       0          0 138.1  384  27.21 16.61 2.367 6e-06  6377  2294       0          0 10.67   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5868         0                   60 11994 4315 .3608\n",
      "\n",
      "19:22:19 | running eval: valid\n",
      "19:22:19 | eval completed in 0.12s\n",
      "19:22:19 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 959.1       0          0 49.16    4 35.75 2.195 6e-06   143  1758       0          0 8.979      .5455   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   60  221 2717\n",
      "\u001b[0m\n",
      "19:22:19 | \u001b[1;32mnew best ppl: 8.979 (previous best was 9.364)\u001b[0m\n",
      "19:22:19 | saving best valid model: from_custom_trained/model\n",
      "19:22:27 | time:494s total_exs:14964 total_steps:61 epochs:3741.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 599.1       0          0 37.44  160  16.51    80 2.531 6.1e-06 12800  2995       0          0 12.57   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4625         0                   61 15360 3594 .2343\n",
      "\n",
      "19:22:27 | running eval: valid\n",
      "19:22:27 | eval completed in 0.12s\n",
      "19:22:27 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  token_acc  \\\n",
      "    19.5    78 932.8       0          0 47.81    4 35.75 2.151 6.1e-06   143  1710       0          0 8.59      .5455   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   61  221 2645\n",
      "\u001b[0m\n",
      "19:22:27 | \u001b[1;32mnew best ppl: 8.59 (previous best was 8.979)\u001b[0m\n",
      "19:22:27 | saving best valid model: from_custom_trained/model\n",
      "19:22:34 | time:501s total_exs:15332 total_steps:62 epochs:3833.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "    14.3     1  5264  1957       0          0 136.8  368  26.17 18.87 1.969 6.2e-06  6944  2582       0          0 7.165   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5507         0                   62 12208 4539 .3729\n",
      "\n",
      "19:22:34 | running eval: valid\n",
      "19:22:35 | eval completed in 0.15s\n",
      "19:22:35 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 937.2       0          0 47.99    4 35.75 2.106 6.2e-06   143  1716       0          0 8.217      .5455   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   62  221 2655\n",
      "\u001b[0m\n",
      "19:22:35 | \u001b[1;32mnew best ppl: 8.217 (previous best was 8.59)\u001b[0m\n",
      "19:22:35 | saving best valid model: from_custom_trained/model\n",
      "19:22:41 | time:508s total_exs:15544 total_steps:63 epochs:3886.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      33     1  6996  2742       0          0 83.08  212  18.05    28 1.203 6.3e-06  5936  2326       0          0 3.33   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7500         0                   63 12932 5068 .3927\n",
      "\n",
      "19:22:41 | running eval: valid\n",
      "19:22:42 | eval completed in 0.12s\n",
      "19:22:42 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 893.8       0          0 45.82    4 35.75 2.064 6.3e-06   143  1638       0          0 7.874      .5524   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   63  221 2534\n",
      "\u001b[0m\n",
      "19:22:42 | \u001b[1;32mnew best ppl: 7.874 (previous best was 8.217)\u001b[0m\n",
      "19:22:42 | saving best valid model: from_custom_trained/model\n",
      "19:22:48 | time:514s total_exs:15828 total_steps:64 epochs:3957.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.52     1  4124  1923       0          0 132.4  284  25.69 17.35  2.02 6.4e-06  4928  2298       0          0 7.536   \n",
      "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
      "        .5739         0                   64 9052 4221 .4676\n",
      "\n",
      "19:22:48 | running eval: valid\n",
      "19:22:48 | eval completed in 0.11s\n",
      "19:22:48 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 947.5       0          0 48.56    4 35.75 2.019 6.4e-06   143  1737       0          0 7.534      .5594   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   64  221 2685\n",
      "\u001b[0m\n",
      "19:22:48 | \u001b[1;32mnew best ppl: 7.534 (previous best was 7.874)\u001b[0m\n",
      "19:22:48 | saving best valid model: from_custom_trained/model\n",
      "19:22:57 | time:523s total_exs:15988 total_steps:65 epochs:3997.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "   14.75     1  2360   504       0          0 34.17  160  15.46 27.61 2.137 6.5e-06  4418 943.5       0          0 8.47   \n",
      "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
      "        .5419         0                   65 6778 1448 .2138\n",
      "\n",
      "19:22:57 | running eval: valid\n",
      "19:22:57 | eval completed in 0.13s\n",
      "19:22:57 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 823.9       0          0 42.23    4 35.75 1.972 6.5e-06   143  1510       0          0 7.183      .5664   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   65  221 2336\n",
      "\u001b[0m\n",
      "19:22:57 | \u001b[1;32mnew best ppl: 7.183 (previous best was 7.534)\u001b[0m\n",
      "19:22:57 | saving best valid model: from_custom_trained/model\n",
      "19:23:06 | time:532s total_exs:16148 total_steps:66 epochs:4037.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   15.24     1  2438 578.3       0          0 37.95  160   13.9 47.47 2.242 6.6e-06  7595  1802       0          0 9.408   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4997         0                   66 10033 2380 .2374\n",
      "\n",
      "19:23:06 | running eval: valid\n",
      "19:23:06 | eval completed in 0.12s\n",
      "19:23:06 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 925.3       0          0 47.43    4 35.75 1.921 6.6e-06   143  1696       0          0 6.826      .5874   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   66  221 2622\n",
      "\u001b[0m\n",
      "19:23:06 | \u001b[1;32mnew best ppl: 6.826 (previous best was 7.183)\u001b[0m\n",
      "19:23:06 | saving best valid model: from_custom_trained/model\n",
      "19:23:14 | time:541s total_exs:16308 total_steps:67 epochs:4077.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      16     1  2560 636.9       0          0 39.81  160  16.03    80 2.287 6.7e-06 12800  3185       0          0 9.85   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .4750         0                   67 15360 3822 .2493\n",
      "\n",
      "19:23:14 | running eval: valid\n",
      "19:23:14 | eval completed in 0.14s\n",
      "19:23:14 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 759.1       0          0 37.92    4 35.75 1.868 6.7e-06   143  1356       0          0 6.477      .6084   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   67  221 2151\n",
      "\u001b[0m\n",
      "19:23:14 | \u001b[1;32mnew best ppl: 6.477 (previous best was 6.826)\u001b[0m\n",
      "19:23:14 | saving best valid model: from_custom_trained/model\n",
      "19:23:21 | time:547s total_exs:16520 total_steps:68 epochs:4130.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  2956       0          0 89.58  212     18    28  1.07 6.8e-06  5936  2508       0          0 2.916   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7857         0                   68 12932 5465 .4234\n",
      "\n",
      "19:23:21 | running eval: valid\n",
      "19:23:21 | eval completed in 0.12s\n",
      "19:23:21 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   953       0          0 48.84    4 35.75 1.819 6.8e-06   143  1747       0          0 6.164      .6154   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   68  221 2700\n",
      "\u001b[0m\n",
      "19:23:21 | \u001b[1;32mnew best ppl: 6.164 (previous best was 6.477)\u001b[0m\n",
      "19:23:21 | saving best valid model: from_custom_trained/model\n",
      "19:23:27 | time:554s total_exs:16784 total_steps:69 epochs:4196.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.53     1  3836  1903       0          0 130.9  264  24.49 17.29 1.611 6.9e-06  4564  2264       0          0 5.008   \n",
      "    token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
      "        .7213         0                   69 8400 4166 .5111\n",
      "\n",
      "19:23:27 | running eval: valid\n",
      "19:23:27 | eval completed in 0.12s\n",
      "19:23:27 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 940.1       0          0 48.18    4 35.75  1.77 6.9e-06   143  1723       0          0 5.868      .6364   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   69  221 2664\n",
      "\u001b[0m\n",
      "19:23:27 | \u001b[1;32mnew best ppl: 5.868 (previous best was 6.164)\u001b[0m\n",
      "19:23:27 | saving best valid model: from_custom_trained/model\n",
      "19:23:34 | time:560s total_exs:16996 total_steps:70 epochs:4249.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  3016       0          0 91.39  212  22.92    28 1.009 7e-06  5936  2559       0          0 2.742   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .7857         0                   70 12932 5575 .4320\n",
      "\n",
      "19:23:34 | running eval: valid\n",
      "19:23:34 | eval completed in 0.12s\n",
      "19:23:34 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   922       0          0 47.25    4 35.75 1.724 7e-06   143  1690       0          0 5.605      .6573   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   70  221 2708\n",
      "\u001b[0m\n",
      "19:23:34 | \u001b[1;32mnew best ppl: 5.605 (previous best was 5.868)\u001b[0m\n",
      "19:23:34 | saving best valid model: from_custom_trained/model\n",
      "19:23:43 | time:569s total_exs:17156 total_steps:71 epochs:4289.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      16     1  2560 606.7       0          0 37.92  160  15.67    80 2.111 7.1e-06 12800  3034       0          0 8.257   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5375         0                   71 15360 3640 .2372\n",
      "\n",
      "19:23:43 | running eval: valid\n",
      "19:23:43 | eval completed in 0.11s\n",
      "19:23:43 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 952.4       0          0 48.81    4 35.75 1.678 7.1e-06   143  1746       0          0 5.354      .6713   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   71  221 2698\n",
      "\u001b[0m\n",
      "19:23:43 | \u001b[1;32mnew best ppl: 5.354 (previous best was 5.605)\u001b[0m\n",
      "19:23:43 | saving best valid model: from_custom_trained/model\n",
      "19:23:49 | time:576s total_exs:17368 total_steps:72 epochs:4342.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      33     1  6996  2965       0          0 89.84  212  16.96    28 .9400 7.2e-06  5936  2516       0          0 2.56   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .8214         0                   72 12932 5481 .4333\n",
      "\n",
      "19:23:49 | running eval: valid\n",
      "19:23:49 | eval completed in 0.11s\n",
      "19:23:49 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 951.1       0          0 48.75    4 35.75 1.635 7.2e-06   143  1743       0          0 5.131      .6783   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   72  221 2697\n",
      "\u001b[0m\n",
      "19:23:49 | \u001b[1;32mnew best ppl: 5.131 (previous best was 5.354)\u001b[0m\n",
      "19:23:49 | saving best valid model: from_custom_trained/model\n",
      "19:23:56 | time:582s total_exs:17580 total_steps:73 epochs:4395.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      33     1  6996  3021       0          0 91.56  212  16.79    28 .9008 7.3e-06  5936  2564       0          0 2.462   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .8214         0                   73 12932 5585 .4328\n",
      "\n",
      "19:23:56 | running eval: valid\n",
      "19:23:56 | eval completed in 0.11s\n",
      "19:23:56 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 953.8       0          0 48.88    4 35.75 1.595 7.3e-06   143  1748       0          0 4.929      .6783   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   73  221 2702\n",
      "\u001b[0m\n",
      "19:23:56 | \u001b[1;32mnew best ppl: 4.929 (previous best was 5.131)\u001b[0m\n",
      "19:23:56 | saving best valid model: from_custom_trained/model\n",
      "19:24:03 | time:590s total_exs:17964 total_steps:74 epochs:4491.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "   14.33     1  5503  1888       0          0 131.8  384   21.3 18.68 1.244 7.4e-06  7175  2462       0          0 3.468   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .8213         0                   74 12678 4350 .3441\n",
      "\n",
      "19:24:03 | running eval: valid\n",
      "19:24:03 | eval completed in 0.12s\n",
      "19:24:03 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 953.5       0          0 48.87    4 35.75 1.556 7.4e-06   143  1748       0          0 4.739      .6853   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   74  221 2702\n",
      "\u001b[0m\n",
      "19:24:03 | \u001b[1;32mnew best ppl: 4.739 (previous best was 4.929)\u001b[0m\n",
      "19:24:03 | saving best valid model: from_custom_trained/model\n",
      "19:24:12 | time:598s total_exs:18124 total_steps:75 epochs:4531.00\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
      "      16     1  2560 602.5       0          0 37.65  160  15.35    80 1.953 7.5e-06 12800  3012       0          0 7.05   \n",
      "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
      "        .5625         0                   75 15360 3615 .2356\n",
      "\n",
      "19:24:12 | running eval: valid\n",
      "19:24:12 | eval completed in 0.12s\n",
      "19:24:12 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78   925       0          0 47.41    4 35.75 1.514 7.5e-06   143  1696       0          0 4.545      .6853   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   75  221 2709\n",
      "\u001b[0m\n",
      "19:24:12 | \u001b[1;32mnew best ppl: 4.545 (previous best was 4.739)\u001b[0m\n",
      "19:24:12 | saving best valid model: from_custom_trained/model\n",
      "19:24:20 | max_train_time elapsed:606.7029550075531s\n",
      "19:24:20 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model)\u001b[0m\n",
      "19:24:20 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: True,checkpoint_activations: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.8/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
      "19:24:20 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 16 --max-train-time -1 --save-every-n-secs 60.0 --save-after-valid True --validation-max-exs 20000 --validation-patience 15 --validation-metric-mode min --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --dropout 0.1 --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --fp16-impl apex --force-fp16-tokens False --learningrate 7.5e-06 --max-lr-steps -1 --warmup-updates -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
      "19:24:20 | loading dictionary from from_custom_trained/model.dict\n",
      "19:24:20 | num words = 54944\n",
      "19:24:21 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "19:24:21 | Loading existing model params from from_custom_trained/model\n",
      "19:24:22 | creating task(s): my_teacher\n",
      " ~~ Loading from valid.txt ~~ \n",
      "19:24:23 | running eval: valid\n",
      "19:24:23 | eval completed in 0.12s\n",
      "19:24:23 | \u001b[1mvalid:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 894.3       0          0 45.83    4 35.75 1.514 7.5e-06   143  1639       0          0 4.545      .6853   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   75  221 2534\n",
      "\u001b[0m\n",
      "19:24:23 | creating task(s): my_teacher\n",
      " ~~ Loading from test.txt ~~ \n",
      "19:24:24 | running eval: test\n",
      "19:24:24 | eval completed in 0.11s\n",
      "19:24:24 | \u001b[1mtest:\n",
      "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  \\\n",
      "    19.5    78 943.9       0          0 48.37    4 35.75 1.514 7.5e-06   143  1730       0          0 4.545      .6853   \n",
      "    token_em  total_train_updates  tpb  tps  \n",
      "           0                   75  221 2675\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'exs': SumMetric(4),\n",
       "  'clen': AverageMetric(19.5),\n",
       "  'ctrunc': AverageMetric(0),\n",
       "  'ctrunclen': AverageMetric(0),\n",
       "  'llen': AverageMetric(35.75),\n",
       "  'ltrunc': AverageMetric(0),\n",
       "  'ltrunclen': AverageMetric(0),\n",
       "  'loss': AverageMetric(1.514),\n",
       "  'ppl': PPLMetric(4.545),\n",
       "  'token_acc': AverageMetric(0.6853),\n",
       "  'token_em': AverageMetric(0),\n",
       "  'exps': GlobalTimerMetric(45.83),\n",
       "  'ltpb': GlobalAverageMetric(143),\n",
       "  'ltps': GlobalTimerMetric(1639),\n",
       "  'ctpb': GlobalAverageMetric(78),\n",
       "  'ctps': GlobalTimerMetric(894.3),\n",
       "  'tpb': GlobalAverageMetric(221),\n",
       "  'tps': GlobalTimerMetric(2534),\n",
       "  'lr': GlobalAverageMetric(7.5e-06),\n",
       "  'total_train_updates': GlobalFixedMetric(75)},\n",
       " {'exs': SumMetric(4),\n",
       "  'clen': AverageMetric(19.5),\n",
       "  'ctrunc': AverageMetric(0),\n",
       "  'ctrunclen': AverageMetric(0),\n",
       "  'llen': AverageMetric(35.75),\n",
       "  'ltrunc': AverageMetric(0),\n",
       "  'ltrunclen': AverageMetric(0),\n",
       "  'loss': AverageMetric(1.514),\n",
       "  'ppl': PPLMetric(4.545),\n",
       "  'token_acc': AverageMetric(0.6853),\n",
       "  'token_em': AverageMetric(0),\n",
       "  'exps': GlobalTimerMetric(48.37),\n",
       "  'ltpb': GlobalAverageMetric(143),\n",
       "  'ltps': GlobalTimerMetric(1730),\n",
       "  'ctpb': GlobalAverageMetric(78),\n",
       "  'ctps': GlobalTimerMetric(943.9),\n",
       "  'tpb': GlobalAverageMetric(221),\n",
       "  'tps': GlobalTimerMetric(2675),\n",
       "  'lr': GlobalAverageMetric(7.5e-06),\n",
       "  'total_train_updates': GlobalFixedMetric(75)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf from_custom_trained\n",
    "!mkdir -p from_custom_trained\n",
    "from parlai.scripts.train_model import TrainModel\n",
    "TrainModel.main(\n",
    "    # similar to before\n",
    "    task='my_teacher', \n",
    "    model='transformer/generator',\n",
    "    model_file='from_custom_trained/model',\n",
    "    \n",
    "    # initialize with a pretrained model\n",
    "    init_model='zoo:blender/blender_90M/model',\n",
    "    \n",
    "    # arguments we get from the pretrained model.\n",
    "    # Unfortunately, these must be looked up separately for each model.\n",
    "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
    "    label_truncate=128, ffn_size=2048, embedding_size=512,model_parallel=True,\n",
    "    activation='gelu', variant='xlm',\n",
    "    dict_lower=True, dict_tokenizer='bpe',\n",
    " dict_file='/usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model.dict',\n",
    "    learn_positional_embeddings=True,\n",
    "    \n",
    "    # some training arguments, specific to this fine-tuning\n",
    "    # use a small learning rate with ADAM optimizer\n",
    "    lr=1e-5, optimizer='adamax',\n",
    "    warmup_updates=100,\n",
    "    # early stopping on perplexity\n",
    "    validation_metric='ppl',\n",
    "    # train at most 10 minutes, and validate every 0.25 epochs\n",
    "    max_train_time=600, validation_every_n_epochs=0.25,\n",
    "    \n",
    "    # depend on our system. Synce we have pretty good AI accelerator it works fine\n",
    "    batchsize=24, fp16=True, fp16_impl='safe',\n",
    "    \n",
    "    # speeds up validation\n",
    "    skip_generation=True,\n",
    "    \n",
    "    # helps us cram more examples into our accelerator at a time\n",
    "    dynamic_batching='full',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73bdce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:40:30 | \u001b[33mOverriding opt[\"task\"] to blended_skill_talk (previously: my_teacher)\u001b[0m\n",
      "19:40:30 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
      "19:40:30 | loading dictionary from from_custom_trained/model.dict\n",
      "19:40:30 | num words = 54944\n",
      "19:40:31 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "19:40:31 | Loading existing model params from from_custom_trained/model\n",
      "19:40:32 | creating task(s): blended_skill_talk\n",
      "19:40:32 | Loading ParlAI text data: /usr/local/lib/python3.8/dist-packages/data/blended_skill_talk/valid.txt\n",
      "19:40:32 | Opt:\n",
      "19:40:32 |     activation: gelu\n",
      "19:40:32 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "19:40:32 |     adam_eps: 1e-08\n",
      "19:40:32 |     add_p1_after_newln: False\n",
      "19:40:32 |     aggregate_micro: False\n",
      "19:40:32 |     allow_missing_init_opts: False\n",
      "19:40:32 |     attention_dropout: 0.0\n",
      "19:40:32 |     batchsize: 24\n",
      "19:40:32 |     beam_block_full_context: True\n",
      "19:40:32 |     beam_block_list_filename: None\n",
      "19:40:32 |     beam_block_ngram: -1\n",
      "19:40:32 |     beam_context_block_ngram: -1\n",
      "19:40:32 |     beam_delay: 30\n",
      "19:40:32 |     beam_length_penalty: 0.65\n",
      "19:40:32 |     beam_min_length: 1\n",
      "19:40:32 |     beam_size: 1\n",
      "19:40:32 |     betas: '[0.9, 0.999]'\n",
      "19:40:32 |     bpe_add_prefix_space: None\n",
      "19:40:32 |     bpe_debug: False\n",
      "19:40:32 |     bpe_dropout: None\n",
      "19:40:32 |     bpe_merge: None\n",
      "19:40:32 |     bpe_vocab: None\n",
      "19:40:32 |     checkpoint_activations: False\n",
      "19:40:32 |     compute_tokenized_bleu: False\n",
      "19:40:32 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "19:40:32 |     datatype: train\n",
      "19:40:32 |     delimiter: '\\n'\n",
      "19:40:32 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "19:40:32 |     dict_endtoken: __end__\n",
      "19:40:32 |     dict_file: from_custom_trained/model.dict\n",
      "19:40:32 |     dict_include_test: False\n",
      "19:40:32 |     dict_include_valid: False\n",
      "19:40:32 |     dict_initpath: None\n",
      "19:40:32 |     dict_language: english\n",
      "19:40:32 |     dict_loaded: True\n",
      "19:40:32 |     dict_lower: True\n",
      "19:40:32 |     dict_max_ngram_size: -1\n",
      "19:40:32 |     dict_maxexs: -1\n",
      "19:40:32 |     dict_maxtokens: -1\n",
      "19:40:32 |     dict_minfreq: 0\n",
      "19:40:32 |     dict_nulltoken: __null__\n",
      "19:40:32 |     dict_starttoken: __start__\n",
      "19:40:32 |     dict_textfields: text,labels\n",
      "19:40:32 |     dict_tokenizer: bpe\n",
      "19:40:32 |     dict_unktoken: __unk__\n",
      "19:40:32 |     display_add_fields: \n",
      "19:40:32 |     display_examples: False\n",
      "19:40:32 |     download_path: None\n",
      "19:40:32 |     dropout: 0.0\n",
      "19:40:32 |     dynamic_batching: full\n",
      "19:40:32 |     embedding_projection: random\n",
      "19:40:32 |     embedding_size: 512\n",
      "19:40:32 |     embedding_type: random\n",
      "19:40:32 |     embeddings_scale: True\n",
      "19:40:32 |     eval_batchsize: None\n",
      "19:40:32 |     eval_dynamic_batching: None\n",
      "19:40:32 |     evaltask: None\n",
      "19:40:32 |     ffn_size: 2048\n",
      "19:40:32 |     final_extra_opt: \n",
      "19:40:32 |     force_fp16_tokens: True\n",
      "19:40:32 |     fp16: True\n",
      "19:40:32 |     fp16_impl: safe\n",
      "19:40:32 |     gpu: -1\n",
      "19:40:32 |     gradient_clip: 0.1\n",
      "19:40:32 |     hide_labels: False\n",
      "19:40:32 |     history_add_global_end_token: None\n",
      "19:40:32 |     history_reversed: False\n",
      "19:40:32 |     history_size: -1\n",
      "19:40:32 |     image_cropsize: 224\n",
      "19:40:32 |     image_mode: raw\n",
      "19:40:32 |     image_size: 256\n",
      "19:40:32 |     inference: greedy\n",
      "19:40:32 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "19:40:32 |     init_opt: None\n",
      "19:40:32 |     interactive_mode: False\n",
      "19:40:32 |     invsqrt_lr_decay_gamma: -1\n",
      "19:40:32 |     is_debug: False\n",
      "19:40:32 |     label_truncate: 128\n",
      "19:40:32 |     learn_positional_embeddings: True\n",
      "19:40:32 |     learningrate: 1e-05\n",
      "19:40:32 |     log_every_n_secs: -1\n",
      "19:40:32 |     log_every_n_steps: 50\n",
      "19:40:32 |     loglevel: info\n",
      "19:40:32 |     lr_scheduler: reduceonplateau\n",
      "19:40:32 |     lr_scheduler_decay: 0.5\n",
      "19:40:32 |     lr_scheduler_patience: 3\n",
      "19:40:32 |     max_train_steps: -1\n",
      "19:40:32 |     max_train_time: 600.0\n",
      "19:40:32 |     metrics: default\n",
      "19:40:32 |     model: transformer/generator\n",
      "19:40:32 |     model_file: from_custom_trained/model\n",
      "19:40:32 |     model_parallel: True\n",
      "19:40:32 |     momentum: 0\n",
      "19:40:32 |     multitask_weights: [1]\n",
      "19:40:32 |     mutators: None\n",
      "19:40:32 |     n_decoder_layers: -1\n",
      "19:40:32 |     n_encoder_layers: -1\n",
      "19:40:32 |     n_heads: 16\n",
      "19:40:32 |     n_layers: 8\n",
      "19:40:32 |     n_positions: 512\n",
      "19:40:33 |     n_segments: 0\n",
      "19:40:33 |     nesterov: True\n",
      "19:40:33 |     no_cuda: False\n",
      "19:40:33 |     num_epochs: -1\n",
      "19:40:33 |     num_examples: 2\n",
      "19:40:33 |     num_workers: 0\n",
      "19:40:33 |     nus: [0.7]\n",
      "19:40:33 |     optimizer: adamax\n",
      "19:40:33 |     output_scaling: 1.0\n",
      "19:40:33 |     override: \"{'task': 'blended_skill_talk', 'model_file': 'from_custom_trained/model', 'num_examples': '2', 'skip_generation': False}\"\n",
      "19:40:33 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "19:40:33 |     person_tokens: False\n",
      "19:40:33 |     rank_candidates: False\n",
      "19:40:33 |     relu_dropout: 0.0\n",
      "19:40:33 |     save_after_valid: False\n",
      "19:40:33 |     save_every_n_secs: -1\n",
      "19:40:33 |     share_word_embeddings: True\n",
      "19:40:33 |     short_final_eval: False\n",
      "19:40:33 |     skip_generation: False\n",
      "19:40:33 |     special_tok_lst: None\n",
      "19:40:33 |     split_lines: False\n",
      "19:40:33 |     starttime: Feb28_19-14\n",
      "19:40:33 |     task: blended_skill_talk\n",
      "19:40:33 |     temperature: 1.0\n",
      "19:40:33 |     tensorboard_log: False\n",
      "19:40:33 |     tensorboard_logdir: None\n",
      "19:40:33 |     text_truncate: 512\n",
      "19:40:33 |     topk: 10\n",
      "19:40:33 |     topp: 0.9\n",
      "19:40:33 |     truncate: -1\n",
      "19:40:33 |     update_freq: 1\n",
      "19:40:33 |     use_reply: label\n",
      "19:40:33 |     validation_cutoff: 1.0\n",
      "19:40:33 |     validation_every_n_epochs: 0.25\n",
      "19:40:33 |     validation_every_n_secs: -1\n",
      "19:40:33 |     validation_every_n_steps: -1\n",
      "19:40:33 |     validation_max_exs: -1\n",
      "19:40:33 |     validation_metric: ppl\n",
      "19:40:33 |     validation_metric_mode: None\n",
      "19:40:33 |     validation_patience: 10\n",
      "19:40:33 |     validation_share_agent: False\n",
      "19:40:33 |     variant: xlm\n",
      "19:40:33 |     verbose: False\n",
      "19:40:33 |     wandb_entity: None\n",
      "19:40:33 |     wandb_log: False\n",
      "19:40:33 |     wandb_name: None\n",
      "19:40:33 |     wandb_project: None\n",
      "19:40:33 |     warmup_rate: 0.0001\n",
      "19:40:33 |     warmup_updates: 100\n",
      "19:40:33 |     weight_decay: None\n",
      "\u001b[1;31m- - - NEW EPISODE: blended_skill_talk- - -\u001b[0;0m\n",
      "\u001b[0myour persona: i work as an electrician.\n",
      "your persona: i always sleep 8 hours a day.\n",
      "Electrician\n",
      "That sounds dangerous. Is it worth doing such a dangerous job?\n",
      "Wekk it is okay is you are well trained.  There are three levels: Apprentice, journeyman and Master.\n",
      "Which level are you at?\u001b[0;0m\n",
      "\u001b[1;94m    labels: I received on-the-job training when i first started\u001b[0;0m\n",
      "\u001b[0;95m     model: i am a apprentice . i am also a master electrician .\u001b[0;0m\n",
      "\u001b[0mThats great! How long have you been doing this work? \u001b[0;0m\n",
      "\u001b[1;94m    labels: For a good number of years now.\u001b[0;0m\n",
      "\u001b[0;95m     model: since i was a kid . i have been working as an electrician since i was a kid .\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.display_model import DisplayModel\n",
    "DisplayModel.main(\n",
    "    task='blended_skill_talk',\n",
    "    model_file='from_custom_trained/model',\n",
    "    num_examples=2,  skip_generation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d46fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:40:44 | \u001b[33mOverriding opt[\"task\"] to blended_skill_talk (previously: my_teacher)\u001b[0m\n",
      "19:40:44 | \u001b[33mOverriding opt[\"beam_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
      "19:40:44 | \u001b[33mOverriding opt[\"beam_context_block_ngram\"] to 3 (previously: -1)\u001b[0m\n",
      "19:40:44 | loading dictionary from from_custom_trained/model.dict\n",
      "19:40:44 | num words = 54944\n",
      "19:40:44 | TransformerGenerator: full interactive mode on.\n",
      "19:40:45 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "19:40:45 | Loading existing model params from from_custom_trained/model\n",
      "19:40:46 | Opt:\n",
      "19:40:46 |     activation: gelu\n",
      "19:40:46 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "19:40:46 |     adam_eps: 1e-08\n",
      "19:40:46 |     add_p1_after_newln: False\n",
      "19:40:46 |     aggregate_micro: False\n",
      "19:40:46 |     allow_missing_init_opts: False\n",
      "19:40:46 |     attention_dropout: 0.0\n",
      "19:40:46 |     batchsize: 24\n",
      "19:40:46 |     beam_block_full_context: True\n",
      "19:40:46 |     beam_block_list_filename: None\n",
      "19:40:46 |     beam_block_ngram: 3\n",
      "19:40:46 |     beam_context_block_ngram: 3\n",
      "19:40:46 |     beam_delay: 30\n",
      "19:40:46 |     beam_length_penalty: 0.65\n",
      "19:40:46 |     beam_min_length: 1\n",
      "19:40:46 |     beam_size: 1\n",
      "19:40:46 |     betas: '[0.9, 0.999]'\n",
      "19:40:46 |     bpe_add_prefix_space: None\n",
      "19:40:46 |     bpe_debug: False\n",
      "19:40:46 |     bpe_dropout: None\n",
      "19:40:46 |     bpe_merge: None\n",
      "19:40:46 |     bpe_vocab: None\n",
      "19:40:46 |     checkpoint_activations: False\n",
      "19:40:46 |     compute_tokenized_bleu: False\n",
      "19:40:46 |     datapath: /usr/local/lib/python3.8/dist-packages/data\n",
      "19:40:46 |     datatype: train\n",
      "19:40:46 |     delimiter: '\\n'\n",
      "19:40:46 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "19:40:46 |     dict_endtoken: __end__\n",
      "19:40:46 |     dict_file: from_custom_trained/model.dict\n",
      "19:40:46 |     dict_include_test: False\n",
      "19:40:46 |     dict_include_valid: False\n",
      "19:40:46 |     dict_initpath: None\n",
      "19:40:46 |     dict_language: english\n",
      "19:40:46 |     dict_loaded: True\n",
      "19:40:46 |     dict_lower: True\n",
      "19:40:46 |     dict_max_ngram_size: -1\n",
      "19:40:46 |     dict_maxexs: -1\n",
      "19:40:46 |     dict_maxtokens: -1\n",
      "19:40:46 |     dict_minfreq: 0\n",
      "19:40:46 |     dict_nulltoken: __null__\n",
      "19:40:46 |     dict_starttoken: __start__\n",
      "19:40:46 |     dict_textfields: text,labels\n",
      "19:40:46 |     dict_tokenizer: bpe\n",
      "19:40:46 |     dict_unktoken: __unk__\n",
      "19:40:46 |     display_add_fields: \n",
      "19:40:46 |     display_examples: False\n",
      "19:40:46 |     display_partner_persona: True\n",
      "19:40:46 |     display_prettify: False\n",
      "19:40:46 |     download_path: None\n",
      "19:40:46 |     dropout: 0.0\n",
      "19:40:46 |     dynamic_batching: full\n",
      "19:40:46 |     embedding_projection: random\n",
      "19:40:46 |     embedding_size: 512\n",
      "19:40:46 |     embedding_type: random\n",
      "19:40:46 |     embeddings_scale: True\n",
      "19:40:46 |     eval_batchsize: None\n",
      "19:40:46 |     eval_dynamic_batching: None\n",
      "19:40:46 |     evaltask: None\n",
      "19:40:46 |     ffn_size: 2048\n",
      "19:40:46 |     final_extra_opt: \n",
      "19:40:46 |     force_fp16_tokens: True\n",
      "19:40:46 |     fp16: True\n",
      "19:40:46 |     fp16_impl: safe\n",
      "19:40:46 |     gpu: -1\n",
      "19:40:46 |     gradient_clip: 0.1\n",
      "19:40:46 |     hide_labels: False\n",
      "19:40:46 |     history_add_global_end_token: None\n",
      "19:40:46 |     history_reversed: False\n",
      "19:40:46 |     history_size: -1\n",
      "19:40:46 |     image_cropsize: 224\n",
      "19:40:46 |     image_mode: raw\n",
      "19:40:46 |     image_size: 256\n",
      "19:40:46 |     include_initial_utterances: False\n",
      "19:40:46 |     include_personas: True\n",
      "19:40:46 |     inference: greedy\n",
      "19:40:46 |     init_model: /usr/local/lib/python3.8/dist-packages/data/models/blender/blender_90M/model\n",
      "19:40:46 |     init_opt: None\n",
      "19:40:46 |     interactive_mode: True\n",
      "19:40:46 |     interactive_task: True\n",
      "19:40:46 |     invsqrt_lr_decay_gamma: -1\n",
      "19:40:46 |     is_debug: False\n",
      "19:40:46 |     label_truncate: 128\n",
      "19:40:46 |     learn_positional_embeddings: True\n",
      "19:40:46 |     learningrate: 1e-05\n",
      "19:40:46 |     local_human_candidates_file: None\n",
      "19:40:46 |     log_every_n_secs: -1\n",
      "19:40:46 |     log_every_n_steps: 50\n",
      "19:40:46 |     log_keep_fields: all\n",
      "19:40:46 |     loglevel: info\n",
      "19:40:46 |     lr_scheduler: reduceonplateau\n",
      "19:40:46 |     lr_scheduler_decay: 0.5\n",
      "19:40:46 |     lr_scheduler_patience: 3\n",
      "19:40:46 |     max_train_steps: -1\n",
      "19:40:46 |     max_train_time: 600.0\n",
      "19:40:46 |     metrics: default\n",
      "19:40:46 |     model: transformer/generator\n",
      "19:40:46 |     model_file: from_custom_trained/model\n",
      "19:40:46 |     model_parallel: True\n",
      "19:40:46 |     momentum: 0\n",
      "19:40:46 |     multitask_weights: [1]\n",
      "19:40:46 |     mutators: None\n",
      "19:40:46 |     n_decoder_layers: -1\n",
      "19:40:46 |     n_encoder_layers: -1\n",
      "19:40:46 |     n_heads: 16\n",
      "19:40:46 |     n_layers: 8\n",
      "19:40:46 |     n_positions: 512\n",
      "19:40:46 |     n_segments: 0\n",
      "19:40:46 |     nesterov: True\n",
      "19:40:46 |     no_cuda: False\n",
      "19:40:46 |     num_epochs: -1\n",
      "19:40:46 |     num_workers: 0\n",
      "19:40:46 |     nus: [0.7]\n",
      "19:40:46 |     optimizer: adamax\n",
      "19:40:46 |     outfile: \n",
      "19:40:46 |     output_scaling: 1.0\n",
      "19:40:46 |     override: \"{'model_file': 'from_custom_trained/model', 'task': 'blended_skill_talk', 'beam_block_ngram': 3, 'beam_context_block_ngram': 3}\"\n",
      "19:40:46 |     parlai_home: /usr/local/lib/python3.8/dist-packages\n",
      "19:40:46 |     person_tokens: False\n",
      "19:40:46 |     rank_candidates: False\n",
      "19:40:46 |     relu_dropout: 0.0\n",
      "19:40:46 |     safe_personas_only: True\n",
      "19:40:46 |     save_after_valid: False\n",
      "19:40:46 |     save_every_n_secs: -1\n",
      "19:40:46 |     save_format: conversations\n",
      "19:40:46 |     share_word_embeddings: True\n",
      "19:40:46 |     short_final_eval: False\n",
      "19:40:46 |     single_turn: False\n",
      "19:40:46 |     skip_generation: True\n",
      "19:40:46 |     special_tok_lst: None\n",
      "19:40:46 |     split_lines: False\n",
      "19:40:46 |     starttime: Feb28_19-14\n",
      "19:40:46 |     task: blended_skill_talk\n",
      "19:40:46 |     temperature: 1.0\n",
      "19:40:46 |     tensorboard_log: False\n",
      "19:40:46 |     tensorboard_logdir: None\n",
      "19:40:46 |     text_truncate: 512\n",
      "19:40:46 |     topk: 10\n",
      "19:40:46 |     topp: 0.9\n",
      "19:40:46 |     truncate: -1\n",
      "19:40:46 |     update_freq: 1\n",
      "19:40:46 |     use_reply: label\n",
      "19:40:46 |     validation_cutoff: 1.0\n",
      "19:40:46 |     validation_every_n_epochs: 0.25\n",
      "19:40:46 |     validation_every_n_secs: -1\n",
      "19:40:46 |     validation_every_n_steps: -1\n",
      "19:40:46 |     validation_max_exs: -1\n",
      "19:40:46 |     validation_metric: ppl\n",
      "19:40:46 |     validation_metric_mode: None\n",
      "19:40:46 |     validation_patience: 10\n",
      "19:40:46 |     validation_share_agent: False\n",
      "19:40:46 |     variant: xlm\n",
      "19:40:46 |     verbose: False\n",
      "19:40:46 |     wandb_entity: None\n",
      "19:40:46 |     wandb_log: False\n",
      "19:40:46 |     wandb_name: None\n",
      "19:40:46 |     wandb_project: None\n",
      "19:40:46 |     warmup_rate: 0.0001\n",
      "19:40:46 |     warmup_updates: 100\n",
      "19:40:46 |     weight_decay: None\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "19:40:46 | creating task(s): blended_skill_talk\n",
      "[ loading personas.. ]\n",
      "\n",
      "  [NOTE: In the BST paper both partners have a persona.\n",
      "         You can choose to ignore yours, the model never sees it.\n",
      "         In the Blender paper, this was not used for humans.\n",
      "         You can also turn personas off with --include-personas False]\n",
      "\n",
      "\u001b[0;34m[context]:\u001b[0;0m \u001b[1myour persona: i graduated valedictorian of my high school class.\n",
      "your persona: my mother stayed at home all day to raise me and my siblings.\u001b[0;0m\n",
      "Enter Your Message: I am feeling sad\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhy is that ?\u001b[0;0m\n",
      "Enter Your Message: A war is going on in Ukrain\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi am sorry to hear that . what is ukraining ?\u001b[0;0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mparlai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteractive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interactive\n\u001b[0;32m----> 2\u001b[0m Interactive\u001b[38;5;241m.\u001b[39mmain(model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_custom_trained/model\u001b[39m\u001b[38;5;124m'\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblended_skill_talk\u001b[39m\u001b[38;5;124m'\u001b[39m, beam_block_ngram\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, beam_context_block_ngram\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/core/script.py:127\u001b[0m, in \u001b[0;36mParlaiScript.main\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_args(args)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kwargs:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_args(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/core/script.py:92\u001b[0m, in \u001b[0;36mParlaiScript._run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_args()\n\u001b[1;32m     91\u001b[0m opt \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_from_parser_and_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/core/script.py:108\u001b[0m, in \u001b[0;36mParlaiScript._run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(opt)\n\u001b[1;32m    107\u001b[0m script\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m parser\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscript\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/scripts/interactive.py:118\u001b[0m, in \u001b[0;36mInteractive.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minteractive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/scripts/interactive.py:93\u001b[0m, in \u001b[0;36minteractive\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Show some example dialogs:\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m world\u001b[38;5;241m.\u001b[39mepoch_done():\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparley\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m world\u001b[38;5;241m.\u001b[39mepoch_done() \u001b[38;5;129;01mor\u001b[39;00m world\u001b[38;5;241m.\u001b[39mget_total_parleys() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# chat was reset with [DONE], [EXIT] or EOF\u001b[39;00m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m world_logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/tasks/interactive/worlds.py:75\u001b[0m, in \u001b[0;36mInteractiveWorld.parley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     agents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mobserve(validate(context_act))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     act \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/parlai/agents/local_human/local_human.py:75\u001b[0m, in \u001b[0;36mLocalHumanAgent.act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m reply[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetID()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     reply_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolorize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter Your Message:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from parlai.scripts.interactive import Interactive\n",
    "Interactive.main(model_file='from_custom_trained/model', task='blended_skill_talk', beam_block_ngram= 3, beam_context_block_ngram= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c69397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
